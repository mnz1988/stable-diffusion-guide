ÿØÿ± ÿ≠ÿßŸÑ Ÿà€åÿ±ÿß€åÿ¥
# √çndice <a name="index"></a>

* [Introducci√≥n](#intro)
* [Google Colab](#colab)
* [Instalaci√≥n Local (Windows + Nvidia)](#install)
* [Conceptos Esenciales](#start)
   1. [Modelos](#model)
   1. [VAEs](#vae)
   1. [Prompts](#prompt)
   1. [Opciones de generaci√≥n](#gen)
* [Extensiones](#extensions)
* [Loras](#lora)
* [Im√°genes Grandes](#upscale)
* [Scripts](#imgscripts)
   * [X/Y/Z Plot](#plot)
   * [Prompt Matrix](#matrix)
   * [Ultimate Upscaler](#ultimate)
* [ControlNet](#controlnet)
* [Entrenamiento de Loras para novatos](#train)
    * [Archivos de entrenamiento](#datasets)
    * [Colab de entrenamiento](#traincolab)
    * [Probar tus resultados](#traintest)
    * [Consejos adicionales](#trainchars)
* [...vtubers?](#vtubers)
 
&nbsp;

# Introducci√≥n <a name="intro"></a>[‚ñ≤](#index)

Stable Diffusion es una poderosa herramienta de generaci√≥n de im√°genes a trav√©s de inteligencia artificial (IA), la cual puedes usar en tu propio hogar. √âsta utiliza "modelos", los cuales son el cerebro de la IA y pueden crear casi cualquier cosa, siempre y cuando alguien los haya entrenado para ello. Los usos m√°s populares son generaci√≥n de arte anime, de fotorealismo, y de contenido para adultos.

Las im√°genes que creas pueden ser usadas para cualquier prop√≥sito, siempre y cuando sigan la licencia del modelo utilizado. Estas im√°genes puede o no que sean "tuyas" en un sentido legal, dependiendo de las leyes de tu pa√≠s, y com√∫nmente es inconcluso. Ni yo ni nadie asociado con Stable Diffusion y sus modelos somos responsables por el contenido que generes, y se te prohibe usar estas herramientas para generar contenido ilegal o da√±ino.

Esta gu√≠a est√° actualizada hasta Marzo de 2023. Una semana es como un a√±o para el desarrollo de IAs, as√≠ que espero que siga siendo √∫til para cuando la leas.

&nbsp;

# Google Colab <a name="colab"></a>[‚ñ≤](#index)

La manera m√°s facil de usar Stable Diffusion es a trav√©s de Google Colab. Con √©l tomas prestado los computadores de Google para usar la IA, con tiempo limitado, com√∫nmente varias horas al d√≠a. Necesitar√°s al menos una cuenta de Google y utilizaremos el Google Drive para guardar tus im√°genes.

Si en su lugar deseas correr el programa en tu propio computador, [baja aqu√≠ ‚ñº](#install).

Aqu√≠ las instrucciones del colab. Estaremos usando el stable-diffusion-webui de Automatic1111.

1. Abre [esta p√°gina con el colab de nocrypt en espa√±ol](https://colab.research.google.com/drive/11-LDA0nvFprS_-g7-ZmTzsDVNpGzRvfO?usp=sharing).

1. Cerca de arriba clickea **Copiar a mi Drive**. Espera que se abra la ventana nueva y cierra la vieja. Ahora tienes tu propio colab el cual puedes configurar a tu gusto, y deber√°s abrir desde tu Google Drive. En caso de actualizaciones deber√°s ver el original.

1. Si deseas puedes activar las siguientes casillas bajo **Configuraciones**: `guardar_imagenes_en_drive, guardar_configuracion_en_drive`.

1. Si ya conoces Stable Diffusion puedes pegar los enlaces a tus recursos deseados en la casilla de texto de `enlaces_adicionales`, bajo **Modelos y otros recursos**. Puede que vayamos a a√±adir enlaces aqu√≠ m√°s adelante en la gu√≠a. Los enlaces deben ser **descargas directas** a cada archivo (idealmente de los sitios civitai o huggingface), y deben separarse por comas.

1. Ahora puedes presionar el bot√≥n de reproducci√≥n a la izquierda, en cualquier lugar dentro de la gran secci√≥n llamada **Empezar aqu√≠ üöÄ**. Espera un par de minutos para que se instale y corra el programa. Ver√°s aparecer mensajes de progreso m√°s abajo. Eventualmente uno de estos mensajes ser√° **Running on public URL** lo cual indica que est√° listo, y puedes abrir ese enlace en una nueva pesta√±a para utilizar Stable Diffusion. **¬°Mant√©n la pesta√±a del colab abierta!** (en tel√©fono intenta el truco en la parte de abajo del colab para evitar que se cierre la pesta√±a)

1. Ahora est√°s listo para hacer algunas im√°genes. Pero eso no es todo, y adem√°s, ¬øqu√© son todas estas opciones? [Baja aqu√≠ ‚ñº](#start) para aprender las bases.

&nbsp;

# Instalaci√≥n Local (Windows + Nvidia) <a name="install"></a>[‚ñ≤](#index)

Para correr Stable Diffusion en tu propio computador necesitar√°s al menos 16 GB de RAM y 4 GB de VRAM (idealmente 8). Por ahora s√≥lo voy a explicar el caso en que uses Windows 10/11 y poseas una tarjeta gr√°fica NVIDIA de serie 10XX o mayor. Mis disculpas si tienes AMD o usas Linux o Mac, pero con ellos es m√°s complejo. Si no cumples estas condiciones a√∫n puedes usar el Google Colab [aqu√≠ arriba ‚ñ≤](#colab).

Aqu√≠ las instrucciones de instalaci√≥n. Utilizaremos un launcher para correr el stable-diffusion-webui de Automatic1111.

1. Obt√©n el instalador m√°s reciente desde [esta p√°gina](https://github.com/EmpireMediaScience/A1111-Web-UI-Installer/releases). 

1. Corre el instalador (dile a Windows que no es un virus) y selecciona una ubicaci√≥n sencilla y accesible en donde instalar. Espera a que termine.

1. Corre el programa, √©ste es el launcher. Ver√°s algunas opciones. Primero activa **medvram** y **xformers**. Si tu gr√°fica tiene 12 GB de VRAM o m√°s no es necesario medvram.

1. En la casilla de texto que dice *Additional Launch Options* pega lo siguiente: `--opt-channelslast --no-half-vae --theme dark` . Si a√±ades m√°s opciones sep√°ralas con espacios.
   * Si tu gr√°fica tiene 4 o 6 GB de VRAM a√±ade `--opt-split-attention-v1` lo cual puede ayudar un poco.
   * Si deseas correr el programa en un dispositivo y usarlo a trav√©s de otro dispositivo en la misma red de WiFi (como en tu tel√©fono), puedes a√±adir `--listen --enable-insecure-extension-access` . Tras iniciar el programa podr√°s conectarte desde el navegador usando la IP local en el puerto 7860. Tambi√©n puedes a√±adir una contrase√±a con `--gradio-auth nombre:contr` .
   * Puedes encontrar todas las opciones [aqu√≠](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings).

1. Presiona **Launch** y espera a que cargue y/o termine de instalar. Cuando termine se abrir√° una ventana en tu navegador.

1. La p√°gina est√° abierta, es tu propio sitio web privado. Aqu√≠ en la pesta√±a principal (txt2img) es donde har√°s casi todas tus im√°genes. Pero primero iremos a la pesta√±a **Settings**, y veremos algunas secciones del lado izquierdo.
   * En la secci√≥n de *Stable Diffusion* baja al final y aumenta el **Clip skip** de 1 a 2. Se dice que produce mejores im√°genes.
   * En la secci√≥n *User Interface*, baja hasta **Quicksettings list** y c√°mbialo a `sd_model_checkpoint, sd_vae` .
   * Vuelve a subir y presiona el gran **Apply settings**, luego **Reload UI**.

1. Ahora est√°s m√°s que listo para generar im√°genes, pero s√≥lo tienes el modelo b√°sico. No es muy bueno, sirve para pinturas entre otras cosas. Adem√°s, ¬øqu√© son todas estas opciones? Ve [aqu√≠ abajo ‚ñº](#start) para aprender las bases.

&nbsp;

# Conceptos Esenciales <a name="start"></a>[‚ñ≤](#index)

Antes o despu√©s de hacer tus primeras im√°genes, querr√°s leer la informaci√≥n de aqu√≠ abajo para mejorar tu experiencia y resultados. Si seguiste las instrucciones de esta gu√≠a, la parte de arriba de tu p√°gina de *Stable Diffusion WebUI* deber√≠a verse parecida a esto:

![Arriba](images/top.png)

Aqu√≠ puedes seleccionar un checkpoint y un VAE. Ahora explicar√© qu√© son ambas cosas y c√≥mo obtenerlas. El colab tiene m√°s opciones aqu√≠ arriba pero puedes ignorarlas.

1. **Modelos** <a name="model"></a>[‚ñ≤](#index)

   El **modelo**, tambi√©n llamado **checkpoint**, es el cerebro de tu IA, dise√±ado para producir cierto tipo de im√°genes. Hay muchas opciones, las cuales puedes encontrar aqu√≠ en huggingface o en [civitai](https://civitai.com). Ya que a√∫n no sabes elegir, estas son mis recomendaciones:
   * Para hacer anime, [7th Heaven Mix](https://civitai.com/models/4669/corneos-7th-heaven-mix) tiene un estilo placentero, est√©ticamente parecido a las pel√≠culas de anime, mientras que [Abyss Orange Mix 3](https://civitai.com/models/9942/abyssorangemix3-aom3) *(__Nota:__ Baja all√≠ y elige la opci√≥n AOM3)* ofrece m√°s realismo con luces suaves, y m√°s lascivia. Personalmente mezcl√© estas dos opciones creando as√≠ [Heaven Orange Mix](https://civitai.com/models/14305/heavenorangemix).
   * Aunque AOM3 es extremadamente capaz de hacer contenido para adultos, el popular modelo de hentai [Grapefruit](https://civitai.com/models/2583/grapefruit-hentai-model) tambi√©n puede cumplir tus deseos.
   * Para arte en general elige [DreamShaper](https://civitai.com/models/4384/dreamshaper), no hay nada que se le acerque en t√©rminos de creatividad. Tambi√©n est√° [Pastel Mix](https://civitai.com/models/5414/pastel-mix-stylized-anime-model), el cual tiene una hermosa y √∫nica est√©tica con un poco de anime.
   * Para el fotorealismo recomiendo [Deliberate](https://civitai.com/models/4823/deliberate). Puede hacer casi cualquier cosa, pero fotos en especial. Muy detallado.
   * El modelo [URPM](https://civitai.com/models/2661/uber-realistic-porn-merge-urpm) es la mayor concentraci√≥n de pornograf√≠a que vas a encontrar.
   
   Si est√°s usando el colab de est√° gu√≠a, algunos de estos est√°n disponibles como opciones. Para otros modelos, debes copiar el **enlace directo a la descarga** y pegarlo en la casilla llamada `enlaces_adicionales`. Separa m√∫ltiples enlaces usando comas.

   En una instalaci√≥n local, com√∫nmente los modelos deben ir dentro de la carpeta `stable-diffusion-webui/models/Stable-diffusion`.

   Una nota importante es que los checkpoints deben estar en formato `.safetensors`, ya que algunos archivos `.ckpt` **pueden** contener virus. Ten cuidado. Adem√°s, cuando elijas modelos a veces ver√°s varias opciones tales comoo fp32, fp16 y pruned. Para generar im√°genes todas funcionan igual, as√≠ que elige el archivo m√°s peque√±o (pruned-fp16). Para mezclar o entrenar modelos se recomienda el archivo m√°s grande.

   **Consejo:** Tras a√±adir el archivo de un recurso a las carpetas del programa, podr√°s encontrarlo tras presionar üîÉ junto al lugar donde lo selecciones.

1. **VAEs** <a name="vae"></a>[‚ñ≤](#index)

   La mayor√≠a de checkpoints no viene con VAE inclu√≠do. El VAE es un peque√±o modelo aparte, el cual "convierte tu imagen a formato humano". Sin un VAE tus im√°genes van a tener malos colores y detalles.

   Si usas el colab de esta gu√≠a, ya deber√≠an estar todos los VAEs instalados.
   
   Pr√°cticamente s√≥lo hay 3 VAEs en circulamiento:
   * [anything vae](https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt), tambi√©n conocido como orangemix vae. Todos los modelos de anime lo utilizan.
   * [vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors), el oficial de Stable Diffusion, utilizado com√∫nmente por modelos realistas.
   * [kl-f8-anime2](https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt), tambi√©n conocido como el vae de Waifu Diffusion, es m√°s viejo y tiene colores m√°s brillantes. Lo utiliza Pastel Mix.

   Si usas el launcher puedes elegir el VAE antes de iniciar el programa, de otra forma los VAEs deben ir en la carpeta `stable-diffusion-webui/models/VAE`.
   
   Si no has seguido esta gu√≠a hasta este punto, dentro de tu p√°gina deber√°s ir a la pesta√±a **Settings**, luego la seccci√≥n **Stable Difussion**, y all√≠ escoger tu VAE y guardar los cambios.

   **Consejo:** Tras a√±adir el archivo de un recurso a las carpetas del programa, podr√°s encontrarlo tras presionar üîÉ junto al lugar donde lo selecciones.

1. **Prompts** <a name="prompt"></a>[‚ñ≤](#index)

   Har√°s casi todas tus im√°genes en la primera pesta√±a de tu p√°gina, **txt2img**. Aqu√≠ ver√°s dos grandes casillas que llamamos *prompt* y *prompt negativo*. Aqu√≠ deber√°s describir lo que deseas que aparezca y no aparezca en tu imagen, **y debe ser en ingl√©s**.  
   Stable Diffusion no es como Midjourney u otros servicios populares, no puedes solo decir lo que quieres, sino que debes ser *muy* espec√≠fico.  
   Debido a esto la mayor√≠a de personas se aferran a alg√∫n prompt que le funcione bien a ellos, muchas veces recomendado por otra persona. Aqu√≠ recomiendo mis propios prompts y prompts negativos:
   
   * Anime
      * `2d, masterpiece, best quality, anime, highly detailed face, highly detailed background, perfect lighting`
      * `EasyNegative, worst quality, low quality, 3d, realistic, photorealistic, (loli, child, teen, baby face), zombie, animal, multiple views, text, watermark, signature, artist name, artist logo, censored`
     
   * Fotorealismo
      * `best quality, 4k, 8k, ultra highres, raw photo in hdr, sharp focus, intricate texture, skin imperfections`
      * `EasyNegative, worst quality, low quality, normal quality, child, painting, drawing, sketch, cartoon, anime, render, 3d, blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art`

   * **EasyNegative:** <a name="promptneg"></a>El prompt negativo que recomiendo utiliza EasyNegative, un *embedding* o "palabra m√°gica" que codifica muchas cosas malas para as√≠ mejorar tus im√°genes. De otra forma tu prompt negativo ser√≠a enorme.
      * Si usas el colab de esta gu√≠a, ya tienes instalado EasyNegative. Sino, es un archivo diminuto que puedes [descargar aqu√≠](https://huggingface.co/datasets/gsdf/EasyNegative/resolve/main/EasyNegative.safetensors) y debes colocar en la carpeta `stable-diffusion-webui/embeddings`. Finalmente debes reiniciar el programa para as√≠ poder usar esta palabra m√°gica.

   Puedes ver una comparaci√≥n de prompts negativos incluyendo EasyNegative m√°s abajo en [Prompt Matrix ‚ñº](#matrixneg).

   ![Prompts](images/prompt.png)

   Despu√©s de un "prompt base" como los que te he mostrado, puedes comenzar a escribir lo que desees. Por ejemplo, `young woman in a bikini in the beach, full body shot`. Tambi√©n puedes a√±adir m√°s t√©rminos negativos, como `old, ugly, futanari, furry`, etc.  
   Puedes guardar tus prompts usando los botones debajo de Generate. Presiona el peque√±o üíæ *Save style* y asigna un nombre al prompt actual. Tras ello podr√°s abrir tus *Styles* para elegirlo, y luego presionar üìã *Apply selected styles to the current prompt* para a√±adirlo.

   <a name="promptweight"></a>Una t√©cnica importante en los prompts es la intensidad. Cuando encierras algo en `(par√©ntesis)`, tendr√° m√°s intensidad o **peso** en tu imagen, dici√©ndole a la IA que esa parte es m√°s importante. El peso normal para todas las palabras es 1, y cada par√©ntesis multiplica por 1.1 (puedes usar varios). Tambi√©n puedes especificar el peso t√∫ mismo, por ejemplo: `(full body:1.4)`. Tambi√©n puedes hacerlo menor a 1 para bajar la intensidad; los `[corchetes]` multiplican por 0.9, pero si quieres ser preciso tambi√©n necesitas par√©ntssis, como `(as√≠:0.5)`.

   Podr√°s notar que la IA es famosamente mala para hacer manos y pies. Con estos buenos prompt mejorar√°n un poco, pero quiz√° debas usar photoshop, inpainting, o t√©cnicas avanzadas como [ControlNet ‚ñº](#controlnet) para perfeccionar tu imagen.

1. **Opciones de generaci√≥n** <a name="gen"></a>[‚ñ≤](#index)

   Ya te ense√±√© a elegir un modelo, VAE y escribir tus prompt, ahora podr√°s saber sobre todo el resto de las opciones disponibles antes de generar una imagen.
   
   ![Parameters](images/parameters.png)

   * **Sampling method:** Es el algoritmo que genera tu imagen, cada uno con resultados distintos. El por defecto de `Euler a` casi siempre es el mejor, y tambi√©n obtendr√°s muy buenos resultados con `DPM++ 2M Karras` y `DPM++ SDE Karras`. Una comparaci√≥n m√°s abajo.
   * **Sampling steps:** La cantidad de pasos, son "calculados" con anticipacic√≥n y por lo tanto m√°s pasos no siempre es mejor. Yo siempre uso 30 pasos, pero de 20 a 50 encontrar√°s resultados consistentemente buenos. Una comparaci√≥n m√°s abajo.
   * **Width and Height:** La resoluci√≥n de 512x512 es lo normal. Si superas el ancho o alto de 768 tu imagen puede ser distorsionada y deformada. Para producir im√°genes m√°s grandes est√° la opci√≥n `Hires fix`.
   * **Batch Count and Batch Size:** El *size* es cu√°ntas im√°genes tu tarjeta gr√°fica producir√° al mismo tiempo, lo cual se limita for su VRAM. El *count* son las repeticiones del valor anterior. Los batches tienen seeds consecutivas, m√°s abajo ver√°s las seeds.
   * **CFG Scale:** "Los valores menores producen resultados m√°s creativos". Casi siempre debes dejarlo en 7, pero de 4 a 10 es un rango aceptable.
   * **Seed:** Un n√∫mero que dicta la generaci√≥n de tu imagen. La misma seed con el mismo prompt y opciones siempre producir√° la misma imagen, salvo detalles menores y algunas excepciones.
  
   **Hires fix:** Esta opci√≥n te permite crear im√°genes m√°s grandes sin problemas. Por lo general se ocupa para duplicar el ancho y alto. Cuando la actives, aparecer√°n m√°s opciones:
   * **Upscaler:** El algoritmo para agrandar la imagen. Se dice que `Latent` crea resultados creativos. Puede que tambi√©n te guste `R-ESRGAN 4x+` y su variante para anime. [M√°s informaci√≥n y comparaciones m√°s abajo ‚ñº](#upscale).
   * **Hires steps:** Recomiendo al menos la mitad de tus pasos normales. M√°s pasos no siempre es mejor, y son bastante lentos, as√≠ que s√© conservador.
   * **Denoising strength:** El valor m√°s importante. Cera de 0.0 tu imagen no tendr√° ning√∫n detalle nuevo. Cerca de 1.0, tu imagen cambiar√° completamente. Recomiendo un valor entre 0.2 y 0.6 dependiendo del caso, lo cual a√±ade suficiente detalle sin *destruir* los detalles existentes que te gusten.
    
   Others:
   * **Restore faces:** Puede mejorar los rostros reales. Nunca lo he necesitado con los prompt de esta gu√≠a y con hires fix.
   * **Tiling:** Sirve para hacer patrones repetitivos como baldosas, no es muy √∫til.
   * **Script:** Te permite acceder a funciones y extensiones muy √∫tiles, [las cuales explico m√°s abajo ‚ñº](#plot). Por ejemplo, X/Y/Z Plot permite comparar una cuadr√≠cula de im√°genes con diferentes opciones. Muy poderoso.

   Aqu√≠ comparo algunos samplers populares y varios cantidades de pasos:

   <details>
   <summary>(Click) Comparaci√≥n de samplers - Fotograf√≠a</summary>

   ![samplers con fotos](images/samplers1.png)
   </details>

   <details>
   <summary>(Click) Comparaci√≥n de samplers - Anime</summary>

   ![samplers con anime](images/samplers2.png)
   </details>

   Corta explicaci√≥n de la comparaci√≥n de arriba: `Euler` es un sampler b√°sico. `DDIM` es m√°s rapido mientras que `DPM++ 2M Karras` es mejorado. Mientras tanto `Euler a` o "ancestral" produce resultados m√°s creativos, y `DPM++ 2S a Karras` algo similar ya que tambi√©n es ancestral. Finalmente `DPM++ SDE Karras` es el m√°s lento y bastante √∫nico. Hay muchos otros samplers pero la mayor√≠a est√°n relacionados.

&nbsp;
  
# Extensiones <a name="extensions"></a>[‚ñ≤](#index)

*Stable Diffusion WebUI* es el programa que estamos ocupando y √©ste permite a√±adir extensiones muy √∫tiles. Para ello dir√≠gete a la pesta√±a **Extensions**, luego a **Install from URL**, y pega all√≠ alguno de estos enlaces de github. Luego presiona *Install* y espera que se instale. Cuando termines ve a **Installed** y presiona *Apply and restart UI*.
 
![Extensiones](images/extensions.png)

Aqu√≠ hay algunas extensiones √∫tiles. Si usas el colab de esta gu√≠a la mayor√≠a ya est√°n instaladas, sino, recomiendo enormemente instalar manualmente las primeras 2.
* [Image Browser (bugfix)](https://github.com/aka7774/sd_images_browser) - Navegador de Im√°genes, permite ver todas las im√°genes wue has creado y r√°pidamente enviarlas con sus par√°metros a txt2img, img2img, etc.
* [TagComplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete) - Completamente esencial para hacer anime, te muestra las tags de booru existentes mientras escribes tu prompt. Los modelos de anime funcionan a trav√©s de estos tags, haciendo de √©sta una de las mejores extensiones. Ojo que no todas las tags funcionan siempre, sobre todo si son poco comunes.
* [ControlNet](https://github.com/Mikubill/sd-webui-controlnet) - Enorme extensi√≥n con [su propia gu√≠a ‚ñº](#controlnet). Te permite analizar cualquier imagen existente y usarla como muestra para guiar tus propias im√°genes. En t√©rminos pr√°cticos, te permite replicar cualquier pose o ambiente que desees.
* [Ultimate Upscale](https://github.com/Coyote-A/ultimate-upscale-for-automatic1111) - Un script usable desde img2img que permite hacer im√°genes enormes aunque tengas poca vram, dividi√©ndolas en secciones aunque sea m√°s lento. [Ver su gu√≠a aqu√≠ ‚ñº](#ultimate).
* [Two-shot](https://github.com/opparco/stable-diffusion-webui-two-shot) - Normalmente no es posible crear escenas de dos personajes, ya que el prompt hace que se fusionen sus caracter√≠sticas. Esta extensi√≥n permite dividir la imagen en: todo, izquierda, derecha; permitiendo as√≠ tener escenas naturales con 2 personajes o temas al mismo tiempo.
* [Dynamic Prompts](https://github.com/adieyal/sd-dynamic-prompts) - Un script para tener prompts semi-aleatorios. Un poco complejo.
* [Model Converter](https://github.com/Akegarasu/sd-webui-model-converter) - Permite convertir modelos de 7 GB o 4 GB a 2 GB, seleccionando  `safetensors`, `fp16`, y `no-ema`. Estos modelos "pruneados" funcionan pr√°cticamente igual para generar im√°genes. La mayor√≠a de modelos hoy en d√≠a vienen en este formato de todas formas.

&nbsp;

# Loras <a name="lora"></a>[‚ñ≤](#index)

Los Loras son una tecnolog√≠a moderna y un tipo de **Extra Network** que permite a√±adir una especie de modelo peque√±o a cualquiera de tus modelos principales. Son similares a los embeddings, uno de los cuales te mostr√© [antes ‚ñ≤](#promptneg), pero los Loras son m√°s grandes y com√∫nmente m√°s capaces. No entrar√© en detalles t√©cnicos.

Un Lora puede representar un personaje, estilo, pose, ropa, o incluso un rostro humano (aunque no estoy de acuerdo con ello). Los checkpoints son bastante capaces para contenido general, pero para detalles como estos es donde comienzan a fallar y necesitar√°s un Lora. Podr√°s descargar Loras desde [civitai](https://civitai.com) u [otros lugares (NSFW)](https://gitgud.io/gayshit/makesomefuckingporn#lora-list) y su tama√±o es de 144 MB por defecto, pero pueden ser tan peque√±os como 1 MB. Los Loras m√°s grandes no son necesariamente mejores. Los Loras vienen en formato `.safetensors` de igual forma que los checkpoints.

Coloca tus archivos de Lora en la carpeta `stable-diffusion-webui/models/Lora`, o si est√°s usando el colab de esta gu√≠a pega el enlace directo a la descarga en la casilla `enlaces_adicionales`. Luego encuentra el bot√≥n üé¥ *Show extra networks* bajo el gran bot√≥n naranjo, el cual abrir√° una nueva secci√≥n de extra networks. Presiona la pesta√±a Lora y presiona **Refresh** para escanear nuevos Loras. Cuando hagas click en uno de tus Loras se a√±adir√° a tu prompt, y se ver√° as√≠: `<lora:archivo:1>` . Siempre se ver√°n as√≠, donde "archivo" es el nombre exacto del archivo en tu sistema (antes de `.safetensors`). Finalmente, el n√∫mero es el peso, lo cual expliqu√© [previamente ‚ñ≤](#promptweight). La mayor√≠a de Loras funcionan con un peso entre 0.5 y 1, y los valores muy grandes pueden "cocinar" tu imagen, especialmente si usas m√°s de uno al mismo tiempo.

![Extra Networks](images/extranetworks.png)

Adem√°s, muchos Loras tendr√°n una "palabra de activaci√≥n" para que tomen efecto, por ejemplo el nombre del personaje en caso de ser un Lora de personaje.

Un ejemplo de Lora es [Thicker Lines Anime Style](https://civitai.com/models/13910/thicker-lines-anime-style-lora-mix), un gran estilo de √°nime cl√°sico si deseas probarlo. No tiene palabra de activaci√≥n.

&nbsp;

# Im√°genes Grandes <a name="upscale"></a>[‚ñ≤](#index)

Como [mencionamos anteriormente ‚ñ≤](#gen), normalmente no debes generar im√°genes sobre 768 de ancho y alto. Debes usar Hires fix, con un "upscaler" (algoritmo) y denoising (intensidad) apropiados. Hires fix est√° limitado por tu VRAM, por lo que te puede interesar [Ultimate Upscaler ‚ñº](#ultimate).

Es posible descargar upscalers adicionales y ponerlos en tu carpeta `stable-diffusion-webui/models/ESRGAN`. As√≠ funcionar√°n con Hires fix, Ultimate Upscaler, y Extras.

El colab de esta gu√≠a viene con varios de estos, incluyendo **Remacri**, uno de los mejores para todo tipo de im√°genes. Se puede encontrar aqu√≠ abajo.

* Algunos upscalers notables [se pueden encontrar aqu√≠](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/ESRGAN).
* LDSR es un upscaler avanzado pero lento, sus dos archivos [se encuentran aqu√≠](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/LDSR) y deben ser puestos en `stable-diffusion-webui/models/LDSR`.
* La [Upscale Wiki](https://upscale.wiki/wiki/Model_Database) contiene docenas de opciones hist√≥ricas.

Aqu√≠ hay algunas comparaciones. Todas fueron hechas con denoising de 0.4. Ojo que algunas diferencias pueden ser completamente debido al azar. Desafortunadamente no puedo mostrar LDSR por limitaciones de VRAM.

<details>
   <summary>(Click) Comparaci√≥n 1: Anime, estilizado, fantas√≠a</summary>

   ![Original](images/upscalers1pre.png)
   ![Comparaci√≥n](images/upscalers1.png)
</details>

<details>
   <summary>(Click) Comparaci√≥n 2: Anime, detallado, iluminaci√≥n suave</summary>

   ![Original](images/upscalers2pre.png)
   ![Comparaci√≥n](images/upscalers2.png)
</details>

<details>
   <summary>(Click) Comparaci√≥n 3: Fotograf√≠a, humano, naturaleza</summary>

   ![Original](images/upscalers3pre.png)
   ![Comparaci√≥n](images/upscalers3.png)
</details>

&nbsp;

# Scripts <a name="imgscripts"></a>[‚ñ≤](#index)

Los Scripts se encuentran al final de tus opciones de generaci√≥n de im√°genes, tanto en txt2img como img2img.

* **X/Y/Z Plot** <a name="plot"></a>[‚ñ≤](#index)

   Capaz de generar una serie de im√°genes, por lo general con la misma seed, pero cambiando algunos otros par√°metros que t√∫ elijas. Permite comparar casi cualquier cosa que desees, como diferentes modelos, partes de tu prompt, sampler, upscaler y mucho m√°s. Puedes tener 1, 2 √≥ 3 par√°metros variables, de all√≠ el X, Y, Z.

   Debes separar tus par√°metros con comas, y cualquier otra cosa puede ir entre medio. El par√°metro m√°s com√∫n es **S/R Prompt**, donde antes de la coma escribes una parte de tu prompt, y cada coma precede una frase que la reemplazar√°. Sabiendo esto, podemos comparar, por ejemplo, el peso/intensidad de un Lora, as√≠:
   
   `<lora:mi lora:0.4>, <lora:mi lora:0.6>, <lora:mi lora:0.8>, <lora:mi lora:1>`

   Aqu√≠ abajo he hecho una comparaci√≥n de diferentes **modelos** (en las columnas) y rostros de diferentes pa√≠ses con **S/R Prompt** (en las filas):
   
   <details>
   <summary>(Click) Ejemplo de X/Y/Z Plot</summary>
   
   ![X Y Z plot de modelos y pa√≠ses](images/XYZplot.png)
   </details>

   **Consejo:** Parece posible hacer S/R incluyendo comas si utilizas comillas de la siguiente forma, sin espacio entre cada coma y comilla: `"frase 1, frase 2","frase 3, fase 4","frase 5, frase 6"`

* **Prompt Matrix** <a name="matrix"></a>[‚ñ≤](#index)

   Un concepto similar al S/R anterior, pero con mayor profundidad. Lo que hace es una cuadr√≠cula, la cual muestra cada posible combinaci√≥n de t√©rminos, donde los t√©rminos estar√°n separados con un `|` en tu prompt. Por ejemplo, `young man|tree, grass|city` - aqu√≠ "young man" siempre ser√° considerado, pero podremos ver qu√© pasa al a√±adir o quitar "tree, grass" y/o "city".
   
   Dentro del script puedes elegir hacerlo con tu prompt o tu negative prompt, y si quieres que los t√©rminos adicionales se introduzcan al inicio o al final.

   <a name="matrixneg"></a>Aqu√≠ hay una comparaci√≥n de los negative prompt que te mostr√© [anteriormente ‚ñ≤](#prompt). Podemos ver c√≥mo EasyNegative afecta la imagen, c√≥mo el resto del negative prompt afecta la imagen, y luego ambos juntos:

   <details>
   <summary>(Click) Ejemplo de Prompt Matrix</summary>
  
   ![Prompt matrix negativo de anime](images/promptmatrix1.png)
   ![Prompt matrix negativo de fotos](images/promptmatrix2.png)
   </details>

   **Tip:** Aumentando el Batch Size puedes crear todo el prompt matrix al mismo tiempo.

* **Ultimate Upscale** <a name="ultimate"></a>[‚ñ≤](#index)

   √âsta es una versi√≥n mejorada de un script b√°sico y debe ser a√±adida como una [extensi√≥n ‚ñ≤](#extensions) para ser usada desde **img2img**. Su prop√≥sito es agrandar una imagen as√≠ a√±adiendo m√°s detalles, sobrepasando el l√≠mite de tu gr√°fica dado que divide la imagen en secciones, aunque esto sea m√°s lento. Aqu√≠ las instrucciones:

   1. Genera tu imagen normalmente hasta un ancho y largo de 768, y aplica Hires fix si es que puedes.

   1. Desde txt2img o la extensi√≥n del navegador de im√°genes, env√≠a directamente la imagen con sus par√°metros a img2img.
   
   1. Ajusta el  **Denoising** entre 0.1 y 0.4. Valores m√°s grandes probablemente introduzcan mutaciones en tu imagen.

   1. Baja a **Scripts** y elije **Ultimate SD Upscale**. Config√∫ralo de la siguiente forma, con el tama√±o y upscaler que desees, y con el **Type "Chess"**:
   
      ![Ultimate upscale](images/ultimate.png)
      
      * Si tienes suficiente VRAM puedes aumentar el **Tile width** y el **Padding**, por ejemplo duplicando ambos. As√≠ ser√° m√°s r√°pido. El **Tile height** puede permanecer en 0.
     
      * No es necesario cambiar el **Seams fix** a menos que la imagen final muestre distorsiones visibles entre cada zona cuadrada.
     
   1. Genera tu imagen y espera que empiece. Podr√°s ver c√≥mo los cuadros se vuelven m√°s n√≠tidos si es que tienes activada la previsualizaci√≥n de im√°genes.
   
&nbsp;

# ControlNet <a name="controlnet"></a>[‚ñ≤](#index)

ControlNet es una tecnolog√≠a reciente extremadamente poderosa. Te permite analizar una imagen para guiar la creaci√≥n de tus propias im√°genes con Stable Diffusion. Veremos qu√© significa esto en un momento.

Si est√°s usando el colab de esta gu√≠a activa la casilla de `todos_modelos_controlnet` antes de iniciarlo. Sino, deber√°s instalar la [extension ControlNet ‚ñ≤](#extensions), luego ir [aqu√≠](https://huggingface.co/webui/ControlNet-modules-safetensors/tree/main) y descargar modelos de controlnet que deber√°s poner en la carpeta `stable-diffusion-webui/extensions/sd-webui-controlnet/models`. Recomiendo los modelos Canny, Depth, Openpose y Scribble, los cuales veremos en un momento.

Voy a demostrar c√≥mo ControlNet puede ser usado. Para ello tomar√© una imagen popular en internet como nuestra "imagen de muestra". No es necesario que me sigas paso a paso, pero puedes descargar las im√°genes y ponerlas en la pesta√±a **PNG Info** para ver los datos de generaci√≥n.

Primero, debes estar en txt2img y bajar para presionar el men√∫ ControlNet. Una vez abierto presiona *Enable*, y elige un *preprocessor* y *model* con el mismo nombre. Para empezar elegir√© Canny para ambos. Finalmente a√±adir√© mi imagen de muestra. Aseg√∫rate de no clickear sobre la imagen de muestra o comenzar√°s a dibujar. Podemos ignorar el resto de las opciones.

![Control Net](images/controlnet.png)

* **Canny**

   El m√©todo Canny extrae los detalles de la imagen de muestra. Es √∫til para imitar todo tipo de im√°genes. Observa:

   <details>
   <summary>(Click) Ejemplo de Canny</summary>
   
   ![Canny preprocessed image](images/canny1.png)
   ![Canny output image](images/canny2.png)
   </details>

* **Depth**

   El m√©todo Depth extrae los elementos 3D de la imagen de muestra. Es de enorme utilidad cuando deseas imitar ambientes complejos y la composici√≥n general de una imagen. Observa:

   <details>
   <summary>(Click) Ejemplo de Depth</summary>
   
   ![Depth preprocessed image](images/depth1.png)
   ![Depth output image](images/depth2.png)
   </details>

* **Openpose**

   El m√©todo Openpose extrae las poses humanas de la imagen de muestra. Es de extrema utilidad para obtener la toma deseada y composici√≥n de uno de tus personajes. Observa:

   <details>
   <summary>(Click) Ejemplo de Openpose</summary>
   
   ![Open Pose preprocessed image](images/openpose1.png)
   ![Open Pose output image](images/openpose2.png)
   </details>

* **Scribble**

   Scribble te permite hacer un bosquejo y convertirlo en una pieza terminada con ayuda de tu prompt. Este es el √∫nico ejemplo de aqu√≠ que no comparte la misma imagen de muestra.

   <details>
   <summary>(Click) Ejemplo de Scribble</summary>
   
   ![Scribble sample image](images/scribble1.png)
   ![Scribble output image](images/scribble2.png)
   </details>

Podr√°s notar que hay 2 resultados para cada m√©todo. El primero es en paso intermedio llamado la "imagen pre-procesada", la cual se usa para producir la imagen final. Puedes entregar una imagen pre-procesada t√∫ mismo, en tal caso deber√°s elegir un preprocessor de *None*. Esto puede ser tremendamente poderoso tomando en cuenta herramientas externas tales como Blender y Photoshop.

En la pesta√±a Settings habr√° una secci√≥n ControlNet donde podr√°s activar *m√∫ltiples controlnets al mismo tiempo*. Un uso particularmente √∫til es cuando uno de ellos es Openpose, para obtener tanto la pose deseada como el ambiente deseado, o con la posici√≥n exacta de manos u otros detalles. Observa:

<details>
<summary>(Click) Ejemplo de Openpose+Canny</summary>
  
![Open Pose + Canny](images/openpose_canny.png)
</details>

Tambi√©n puedes usar ControlNet en img2img, en tal caso la imagen de entrada y la imagen de muestra ambas tendr√°n ciertos efectos en el resultado. No tengo mucha experiencia con este m√©todo.

Adem√°s, existen la version **diff** de los modelos de controlnet, los cuales producen resultados ligeramente distintos. Puedes [probarlos](https://civitai.com/models/9868/controlnet-pre-trained-difference-models) si deseas, pero yo no lo he hecho.

&nbsp;

# Entrenamiento de Loras para novatos <a name="train"></a>[‚ñ≤](#index)

Entrenar un [Lora ‚ñ≤](#lora) t√∫ mismo es una especie de logro. No es la gran cosa, pero hay muchas variables involucradas, y mucho trabajo dependiendo de las t√©cnicas que utilices. Es una mezcla entre un arte y una ciencia.

Puedes entranar Loras en tu propio computador si tienes al menos 8 GB de VRAM. Sin embargo utilizar√© un documento de Google Colab por motivos educacionales.

Tengo una [gu√≠a m√°s actualizada y detallada](https://civitai.com/models/22530) en ingl√©s.

1. **Archivos de entrenamiento** <a name="datasets"></a>[‚ñ≤](#index)
  
   Esta es la mayor parte del entrenamiento de Loras. Necesitar√°s recopilar un "dataset" o archivos de entrenamiento, los cuales consisten en im√°genes y sus correspondientes descripciones (con tags en el caso de anime).

   **Nuevo:** Puedes usar mi [colab preparador de lora](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Dataset_Maker.ipynb) para recopilar y tagear cientos de im√°genes en solo minutos.

   De otra forma:

   1. Encuentra im√°genes online que representens el personaje/concepto/estilo que deseas entrenar, posiblemente en sitios tales como [gelbooru](https://gelbooru.com/). Aseg√∫rate que sean im√°genes de calidad decente en diferentes √°ngulos, escenas, ropa, etc. Necesitas al menos 10 im√°genes, recomiendo 20 o m√°s, y por lo general mientras m√°s mejor.
      * Opcionalmente, puedes instalar [Grabber](https://github.com/Bionus/imgbrd-grabber/releases) para descargar cientos de im√°genes autom√°ticamente. Recomiendo buscar en gelbooru y pixiv, para un personaje ser√≠a con estas tags: `1girl solo character_name score:>10 -rating:explicit` (lo expl√≠cito suele ser raro y por lo tanto se puede excluir).
  
   1. Crea los archivos de texto junto a cada imagen, con el mismo nombre de archivo. Puedes escribir los tags t√∫ mismo, aunque puede ser lento y poco preciso. Si son fotograf√≠as, descr√≠belas en detalle con oraciones simples.
      * Opcionalmente puedes agregar la [extensi√≥n Tagger](https://github.com/toriato/stable-diffusion-webui-wd14-tagger) para que analice tus im√°genes y cree las tags de anime por ti. Las instrucciones son as√≠: A√±ade y activa la extensi√≥n, y reinicia el programa. Luego dir√≠gete a la nueva pesta√±a **Tagger**, luego a *atch from directory*, y selecciona la carpeta con tus im√°genes. Pon el *output name* como `[name].txt` y el threshold a 0.2 o mayor (√©ste es la precisi√≥n de las tags). Finalmente presiona **Interrogate** y se crear√°n tus archivos de texto.
    
   1. Una vez que tus im√°genes y descripciones est√©n listas, ponlas en una misma carpeta y sigue al siguiente paso.
    
1. **Colab de entrenamiento** <a name="traincolab"></a>[‚ñ≤](#index) ![Trainer colab](images/trainercollab_spanish.png)

   Utilizaremos [mi nuevo colab entrenador](https://colab.research.google.com/github/hollowstrawberry/kohya-colab/blob/main/Spanish_Lora_Trainer.ipynb). Usaremos las siguientes configuraciones:

   * **‚ñ∂Ô∏èInicio**
   
      Puedes darle cualquier nombre a tu proyecto, pero sin espacios. Luego, debes crear las siguientes carpetas en tu Google Drive: `lora_training/datasets/nombre_de_proyecto` - Aqu√≠ subir√°s todas tus im√°genes y textos. Por ejemplo, mi proyecto se llama "ina": ![carpetas](images/drivefolders.png)

      Tambi√©n puedes cambiar el modelo base, pero para esta gu√≠a utilizaremos `animefull-final` ya que es la base de casi todos los modelos anime y produce los resultados m√°s consistentes. Si deseas entrenar con fotograf√≠as cambia el modelo a `sd-v1-5` o al modelo realista que desees utilizar.

   * **‚ñ∂Ô∏èArchivos**

      Aqu√≠ hay algunas opciones sobre c√≥mo tus archivos ser√°n usados en el entrenamiento, pero esta vez no necesitas cambiar nada. √âchales un vistazo si deseas.
     
   * **‚ñ∂Ô∏èPasos**
   
      Tu n√∫mero de repeticiones y epochs son muy importantes. Recomiendo que tu cantidad de im√°genes multiplicada por las repeticiones est√© entre 200 y 400, ya que da resultados consistentes. Luego elige entre 10 y 30 epochs dependiendo de cu√°nto deseas entrenar, podemos probar con 15.
   
   * **‚ñ∂Ô∏èEntrenamiento**
   
      El `aprendizaje_unet` es el par√°metro m√°s importante. 5e-4 es el valor por defecto y lo recomiendo para personajes, pero puedes probar 1e-3 si tienes pocas im√°genes y 1e-4 si tienes muchas y/o deseas entrenar m√°s tiempo. Puedes ignorar el resto esta vez.

   * **‚ñ∂Ô∏èListo**
   
      Ahora puedes apretar el bot√≥n circular de la izquierda para correr el colab. Primero realizar√° la instalaci√≥n y luego comenzar√°. Revisa el progreso en los resultados de abajo, deber√≠a tomar de 20 a 60 minutos. ¬°Buena suerte! Si encuentras un error ve si puedes solucionarlo o si necesitas ayuda online. Tambi√©n puedes contactarme.

1. **Probar tus resultados** <a name="traintest"></a>[‚ñ≤](#index)

   Ha pasado un rato y tu Lora termin√≥ de entrenar/cocinar. Ve y desc√°rgalo de la carpeta `lora_training/output` en tu google drive. Pero ver√°s que hay m√°s de uno; por defecto, se guarda una copia de tu Lora cada 2 epochs, permiti√©ndote as√≠ comparar su progreso. Si entrenas tu Lora por muchos epochs, podr√°s identificar el punto √≥ptimo entre que est√© "crudo" o "recocido".

   Cuando un Lora est√° "crudo", no alcanzar√° a imitar tus datos de entrenamiento. Cuando est√° "recocido", imita tus datos de entrenamiento *demasiado*, y comienza a distorsionar tus im√°genes. Si tu dataset o par√°metros estan mal, ¬°puede que est√© crudo y recocido al mismo tiempo!

   Usando lo aprendido en [X/Y/Z Plot ‚ñ≤](#plot), podemos hacer una comparaci√≥n del progreso de nuestro Lora:

   ![Comparaci√≥n del resultado de Lora](images/loratrain.png)

   Mira eso, ¬°se vuelve cada vez m√°s detallado! La √∫ltima imagen no tiene ning√∫n Lora para comparar. Este parece ser un Lora de personaje exitoso, pero necesitar√≠amos probar una variedad de seeds, ropas y escenas para estar seguros.

   * Si tus resultados no funcionan, puede que hayas entrenado muy poco tiempo o m√°s probablemente tu tasa de aprendizaje era muy peque√±a (intenta 5e-4 o en casos extremos 1e-3).
   * Si tus resultados est√°n distorsionados, intenta bajar la intensidad de tu lora entre 0.5 y 0.8. Si siguen distorsionados o deja de funcionar, y epochs anteriores tampoco funcionan, entonces se te quem√≥ el Lora y debes intentar una menor tasa de aprendizaje (1e-4 o 1e-5).
   * Si funciona bien pero tu personaje no puede cambiar de ropa/posici√≥n, tus im√°genes de entrenamiento eran muy similares o sus tags estaban mal.
   * Si funciona bien pero el estilo se ve mal o con mal sombreado, puede que est√©s usando un modelo muy avanzado. Recomiendo `animefull-final-pruned` si puedes encontrarlo.
   
   Despu√©s de acostumbrarse a hacer Loras, e interactuar on la comunidad y sus variados recursos, estar√°s listo para usar otro m√©todo m√°s avanzado como el [colab original todo-en-uno de kohya](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Buena suerte.

* **Consejos adicionales** <a name="trainchars"></a>[‚ñ≤](#index)

   La parte m√°s importante para un personaje son los tags. Claro que necesitas im√°genes con variadas poses y lugares, pero si las descripciones est√°n mal no servir√° de nada.

   Cuando entrenas un personaje o concepto deber√≠as definir una **palabra de activaci√≥n**, y ajustar el valor de `keep_tokens` a 1. Una palabra de activaci√≥n es como podremos invocar a tu Lora para que funcione. Habiendo hecho eso, **algunas personas recomiendan** quitar o "limpiar" las tags que son intr√≠nsicas a tu personaje o concepto, tales como el color de pelo y ojos. Por ejemplo, si una chica siempre tiene orejas de gato, quieres quitar las tags tales como `animal ears, animal ear fluff, cat ears`, y as√≠ √©stas ser√°n "absorbidas" por tu palabra de activaci√≥n. Esto har√° tu Lora m√°s f√°cil de usar pero menos flexible.
   * Puedes usar [mi preparador de lora](https://colab.research.google.com/drive/17v2DeLO8VOjgD9PRwBtRkX7Qyq2WjB_t) o [la extensi√≥n Tag Editor](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor) para a√±adir una palabra de activaci√≥n a todos tus archivos al mismo tiempo. 
   
   Esta "absorci√≥n" de detalles no entregados por los tags es la forma en que los Loras funcionan en general, ya que logran aprender y representar los detalles imperceptibles o dif√≠ciles de explicar tales como el rostro, acccesorios, composici√≥n, etc. Tambi√©n puedes limpiar tags de ropa redundantes, como borrar "red tie" y dejar "tie". Incluso puedes tener una palabra de activaci√≥n distinta para cada atuendo que tu personaje usa com√∫nmente, como personaje-normal, personaje-bikini, etc. Pero no es la √∫nica forma de hacerlo. En cualquier caso, si tus tags son correctas, tu personaje deber√≠a poder cambiar de ropa f√°cilmente.

   Mientras tanto, los Loras de estilo no necesitan palabra de activaci√≥n, ya que deseamos que siempre est√©n activos. Absorber√°n el estilo art√≠stico de forma natural, y funcionar√° con variados pesos.

&nbsp;

# ...vtubers? <a name="vtubers"></a>[‚ñ≤](#index)

Y as√≠ llegamos la final de la gu√≠a. Gracias por leer. Si tienes correcciones o contribuciones puedes abrir un Issue o un Pull Request en esta p√°gina y echar√© un vistazo pronto.

Tengo [otra p√°gina dedicada a Loras de vtubers, en especial Hololive](https://huggingface.co/hollowstrawberry/holotard). Si es que es de tu inter√©s.

&nbsp;

# Index <a name="index"></a>

* [Introducci√≥n](#intro)
* [Google Collab](#collab)
* [Instalaci√≥n Local (Windows + Nvidia)](#install)
* [Conceptos Esenciales](#start)
   1. [Modelos](#model)
   1. [VAEs](#vae)
   1. [Prompts](#prompt)
   1. [Opciones de generaci√≥n](#gen)
* [Extensiones](#extensions)
* [Loras](#lora)
* [Im√°genes Grandes](#upscale)
* [Scripts](#scripts)
   * [X/Y/Z Plot](#plot)
   * [Prompt Matrix](#matrix)
   * [Ultimate Upscaler](#ultimate)
* [ControlNet](#controlnet)
* [Entrenamiento de Loras para novatos](#train)
    * [Archivos de entrenamiento](#datasets)
    * [Opciones de entrenamiento](#trainparams)
    * [Probar tus resultados](#traintest)
    * [Consejos adicionales](#trainchars)
* [...vtubers?](#vtubers)
 
&nbsp;

# Introducci√≥n <a name="intro"></a>[‚ñ≤](#index)

Stable Diffusion es una poderosa herramienta de generaci√≥n de im√°genes a trav√©s de inteligencia artificial (IA), la cual puedes usar en tu propio hogar. √âsta utiliza "modelos", los cuales son el cerebro de la IA y pueden crear casi cualquier cosa, siempre y cuando alguien los haya entrenado para ello. Los usos m√°s populares son generaci√≥n de arte anime, de fotorealismo, y de contenido para adultos.

Las im√°genes que creas pueden ser usadas para cualquier prop√≥sito, siempre y cuando sigan la licencia del modelo utilizado. Estas im√°genes puede o no que sean "tuyas" en un sentido legal, dependiendo de las leyes de tu pa√≠s, y com√∫nmente es inconcluso. Ni yo ni nadie asociado con Stable Diffusion y sus modelos somos responsables por el contenido que generes, y se te prohibe usar estas herramientas para generar contenido ilegal o da√±ino.

Esta gu√≠a est√° actualizada a Marzo de 2023. Una semana es como un a√±o para el desarrollo de IAs, as√≠ que espero que siga siendo √∫til para cuando la leas.

&nbsp;

# Google Collab <a name="collab"></a>[‚ñ≤](#index)

La manera m√°s facil de usar Stable Diffusion es a trav√©s de Google Collab. Con √©l tomas prestado los computadores de Google para usar la IA, con tiempo limitado, com√∫nmente varias horas al d√≠a. Necesitar√°s al menos una cuenta de Google y utilizaremos el Google Drive para guardar tus im√°genes.

Si deseas correr el programa en tu propio computador, [baja aqu√≠ ‚ñº](#install).

1. Abre [esta p√°gina](https://colab.research.google.com/drive/1wEa-tS10h4LlDykd87TF5zzpXIIQoCmq).

1. Cerca del inicio clickea **Copiar a mi Drive**. Espera que se abra la ventana nueva y cierra la vieja. Ahora tienes tu propio collab el cual puedes configurar a tu gusto, y deber√°s abrir desde tu Google Drive. En caso de actualizaciones deber√°s ver el original.

1. Activa las siguientes casillas en **Configurations**: `output_to_drive, configs_in_drive, no_custom_theme`. Luego, activa las siguientes casillas en **Models, etc**: `anything_vae`, `wd_vae`, `sd_vae`.

1. Si ya conoces Stable Diffusion puedes pegar los enlaces a tus recursos deseados en la casilla de  `custom_urls`. Vamos a a√±adir enlaces aqu√≠ m√°s adelante en la gu√≠a. Los enlaces deben ser **descargas directas** a cada archivo (idealmente de los sitios civitai o huggingface), y deben separarse por comas.

1. Presiona el bot√≥n de reproducci√≥n a la izquierda, en cualquier lugar dentro de la gran secci√≥n llamada **Start üöÄ**. Espera un par de minutos para que se instale y corra el programa. Ver√°s aparecer mensajes de progreso m√°s abajo. Eventualmente uno de estos mensajes ser√° un **public link** lo cual indica que est√° listo, y puedes abrir este enlace en una nueva pesta√±a para utilizar Stable Diffusion. **Mant√©n la pesta√±a del collab abierta!** (esto puede ser dif√≠cil si intentas usarlo desde un tel√©fono)

1. Ahora puedes hacer algunas im√°genes decentes gracias al modelo por defecto llamado **Anything 4.5**. Pero podemos hacer m√°s que ello, y adem√°s, ¬øque son todas estas opciones? [Baja aqu√≠ ‚ñº](#start) para empezar.

&nbsp;

# Instalaci√≥n Local (Windows + Nvidia) <a name="install"></a>[‚ñ≤](#index)

Esperando traducci√≥n.

To run Stable Diffusion on your own computer you'll need at least 16 GB of RAM and 4 GB of VRAM (preferably 8). I will only cover the case where you are running Windows 10/11 and using an NVIDIA graphics card series 16XX, 20XX or 30XX (though 10XX also work). My apologies to AMD, Linux, and Mac users, but their cases are harder to cover. If you don't meet the hardware requirements, you can just proceed with the Google Collab method [above](#collab).

1. Get the latest release from [this page](https://github.com/EmpireMediaScience/A1111-Web-UI-Installer/releases). 

1. Run the installer, choose a simple location to install to, and wait for it to finish.

1. Run the program. You will see a few options. First, turn on **medvram** and **xformers**. You may skip medvram if you have 12 or more GB of VRAM.

1. Set your *Additional Launch Options* to: `--opt-channelslast --no-half-vae --theme dark` . Any extra options should be separated by spaces.
    * If your graphics card has less than 8 GB of VRAM, add `--opt-split-attention-v1` as it may lower vram usage even further.
    * If you want to run the program from your computer but want to use it in another device, such as your phone, add `--listen`. After launching, use your computer's local IP in the same WiFi network to access the interface.
    * Full list of possible parameters [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings)

1. Click **Launch** and wait for a browser window to open with the interface. It may take a while the first time.

1. The page is now open. It's your own private website. The starting page is where you can make your images. But first, we'll go to the **Settings** tab. There will be sections of settings on the left.
    * In the *Stable Diffusion* section, scroll down and increase **Clip Skip** from 1 to 2. This is said to produce better images, specially for anime.
    * In the *User Interface* section, scroll down to **Quicksettings list** and change it to `sd_model_checkpoint, sd_vae`
    * Scroll back up, click the big orange **Apply settings** button, then **Reload UI** next to it.

1. You are more than ready to generate some images, but you only have the basic model available. It's not great, at most it can make some paintings. Also, what are all of these options? See [below ‚ñº](#start) to get started.

&nbsp;

# Conceptos Esenciales <a name="start"></a>[‚ñ≤](#index)

Antes o despu√©s de hacer tus primeras im√°genes, querr√°s leer la informaci√≥n de aqu√≠ abajo para mejorar tu experiencia y resultados. Si seguiste las instrucciones de arriba, la parte de arriba de tu p√°gina de Stable Diffusion deber√≠a verse parecida a esto:

![Arriba](images/top.png)

Aqu√≠ puedes seleccionar un modelo y un VAE. Ahora explicar√© qu√© son ambas cosas y c√≥mo obtenerlas. El collab tiene m√°s opciones aqu√≠ arriba pero puedes ignorarlas.

1. **Modelos** <a name="model"></a>[‚ñ≤](#index)

   El **modelo**, tambi√©n llamado **checkpoint**, es el cerebro de tu IA, dise√±ado para producir cierto tipo de im√°genes. Hay muchas opciones, las cuales puedes encontrar aqu√≠ en huggingface o en [civitai](https://civitai.com). Ya que a√∫n no sabes elegir, estas son mis recomendaciones:
   * Para hacer anime, [7th Heaven Mix](https://civitai.com/models/4669/corneos-7th-heaven-mix) tiene un estilo est√©ticamente parecido a las pel√≠culas de anime, mientras que [Abyss Orange Mix 3](https://civitai.com/models/9942/abyssorangemix3-aom3) *(__Nota:__ Baja y elige la opci√≥n AOM3)* ofrece m√°s realismo con luces suaves, y m√°s lascivia. Personalmente mezcl√© estas dos opciones creando as√≠ [Heaven Orange Mix](https://civitai.com/models/14305/heavenorangemix).
   * Aunque AOM3 es extremadamente capaz de hacer contenido para adultos, el popular modelo de hentai [Grapefruit](https://civitai.com/models/2583/grapefruit-hentai-model) tambi√©n puede cumplir tus deseos.
   * Para arte en general elige [DreamShaper](https://civitai.com/models/4384/dreamshaper), no hay nada parecido en t√©rminos de creatividad. Tambi√©n est√° [Pastel Mix](https://civitai.com/models/5414/pastel-mix-stylized-anime-model), el cual tiene una hermosa y √∫nica est√©tica con un poco de anime.
   * Para el fotorealismo recomiendo [Deliberate](https://civitai.com/models/4823/deliberate). Puede hacer casi cualquier cosa, pero fotos en especial. Muy detallado.
   * El modelo [URPM](https://civitai.com/models/2661/uber-realistic-porn-merge-urpm) es la mayor concentraci√≥n de pornograf√≠a que vas a encontrar.
   
   Si est√°s usando el collab en est√° gu√≠a, copia el **enlace directo a la descarga** y p√©galo en la casilla llamada `custom_urls`. Separa m√∫ltiples enlaces usando comas.

   En una instalaci√≥n local, com√∫nmente los modelos deben ir dentro de la carpeta `stable-diffusion-webui/models/Stable-diffusion`.

   Una nota importante es que los checkpoints deben estar en formato `.safetensors`, ya que algunos archivos `.ckpt` **pueden** contener virus. Ten cuidado. Adem√°s, cuando eligas modelos a veces ver√°s varias opciones tales comoo fp32, fp16 y pruned. Para generar im√°genes todas funcionan igual, as√≠ que elige el archivo m√°s peque√±o (pruned-fp16) Para mezclar o entrenar modelos se recomienda el archivo m√°s grande.

   **Consejo:** Tras a√±adir el archivo de un recurso a las carpetas del programa, podr√°s encontrarlo tras presionar üîÉ junto al lugar donde lo selecciones.

1. **VAEs** <a name="vae"></a>[‚ñ≤](#index)

   La mayor√≠a de modelos no viene con VAE inclu√≠do. El VAE es un peque√±o modelo aparte, el cual "convierte tu imagen a formato humano". Sin un VAE tus im√°genes van a tener malos colores y detalles.

   Si usas el collab de esta gu√≠a, te hice elegir todos los VAEs antes de iniciar el programa.
   
   Pr√°cticamente s√≥lo hay 3 VAEs en circulamiento:
   * [anything vae](https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt), tambi√©n conocido como orangemix vae. Todos los modelos de anime lo utilizan.
   * [vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors), el oficial de Stable Diffusion, utilizado com√∫nmente por modelos realistas.
   * [kl-f8-anime2](https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt), tambi√©n conocido como el vae de Waifu Diffusion, es m√°s viejo y tiene colores m√°s brillantes. Lo utiliza Pastel Mix.

   Si usas el launcher puedes elegir el VAE antes de iniciar el programa, de otra forma los VAEs deben ir en la carpeta `stable-diffusion-webui/models/VAE`.
   
   Si no ha seguido esta gu√≠a hasta este punto, dentro de tu p√°gina deber√°s ir a la pesta√±a **Settings**, luego la seccci√≥n **Stable Difussion**, y all√≠ escoger tu VAE.

   **Consejo:** Tras a√±adir el archivo de un recurso a las carpetas del programa, podr√°s encontrarlo tras presionar üîÉ junto al lugar donde lo selecciones.

1. **Prompts** <a name="prompt"></a>[‚ñ≤](#index)

   Har√°s casi todas tus img√°genes en la primera pesta√±a de tu p√°gina, **txt2img**. Aqu√≠ ver√°s dos grandes casillas llamadas *prompt* y *prompt negativo*. Aqu√≠ deberas describir lo que quieres y no quieres que aparezca en tu imagen, **y debe ser en ingl√©s**.  
   Stable Diffusion no es como Midjourney u otros servicios populares, no puedes solo decir lo que quieres, sino que debes ser *muy* espec√≠fico.  
   Debido a esto la mayor√≠a de personas se aferran a alg√∫n prompt que le funcione bien a ellos, muchas veces recomendado por otra persona. Aqu√≠ recomiendo mis propios prompts y prompts negativos:
   
   * Anime
      * `2d, masterpiece, best quality, anime, highly detailed face, highly detailed eyes, highly detailed background, perfect lighting`
      * `EasyNegative, worst quality, low quality, 3d, realistic, photorealistic, (loli, child, teen, baby face), zombie, animal, multiple views, text, watermark, signature, artist name, artist logo, censored`
     
   * Fotorealismo
      * `best quality, 4k, 8k, ultra highres, (realistic, photorealistic, RAW photo:1.4), (hdr, sharp focus:1.2), intricate texture, skin imperfections`
      * `EasyNegative, worst quality, low quality, normal quality, child, painting, drawing, sketch, cartoon, anime, render, 3d, blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art`

   * **EasyNegative:** <a name="promptneg"></a>El prompt negativo que recomiendo utiliza EasyNegative, un *embedding* o "palabra m√°gica" que codifica muchas cosas malas para as√≠ mejorar tus im√°genes. De otra forma tu prompt negativo ser√≠a enorme.
      * Es un archivo diminuto que puedes [descargar aqu√≠](https://huggingface.co/datasets/gsdf/EasyNegative/resolve/main/EasyNegative.safetensors). Si usas el collab de esta gu√≠a puedes pegar el enlace en la casilla `custom_urls`. De otra forma debe ir en la carpeta `stable-diffusion-webui/embeddings`, y luego debes reiniciar el programa para as√≠ poder usar esta palabra m√°gica.

   Puedes ver una comparaci√≥n de prompts negativos incluyendo EasyNegative m√°s abajo en [Prompt Matrix ‚ñº](#matrixneg).

   ![Prompts](images/prompt.png)

   Despu√©s de un "prompt base" como los que te he mostrado, puedes comenzar a escribir lo que desees. Por ejemplo, `young woman in a bikini in the beach, full body shot`. Tambi√©n puedes a√±adir m√°s t√©rminos negativos, como `old, ugly, futanari, furry`, etc.  
   Puedes guardar tus prompts usando los botones debajo de Generate. Presiona el peque√±o üíæ *Save style* y asigna un nombre al prompt actual. Tras ello podr√°s abrir tus *Styles* para elegirlo, y luego presionar üìã *Apply selected styles to the current prompt* para a√±adirlo.

   <a name="promptweight"></a>Debes saber que cuando encierras algo en `(par√©ntesis)`, tendr√° m√°s "√©nfasis/peso/intensidad" en tu imagen, lo cual equivale al valor de `1.1`. El valor base para todas las palabras es 1, y cada par√©ntesis multiplica por 1.1 nuevamente. Tambi√©n puedes especificar el peso t√∫ mismo, por ejemplo: `(full body:1.4)`. Tambi√©n puedes ir menor a 1 para quitar √©nfasis; los `[corchetes]` multiplican por 0.9, pero si quieres mucho menos que eso debes utilizar par√©ntesis nuevamente, como `(as√≠:0.5)`.

   Podr√°s notar que la IA es famosamente mala para hacer manos y pies. Con estos buenos prompt mejorar√°n un poco, pero quiz√° debas usar photoshop, inpainting, o t√©cnicas avanzadas como [ControlNet ‚ñº](#controlnet) para perfeccionar tu imagen.

1. **Opciones de generaci√≥n** <a name="gen"></a>[‚ñ≤](#index)

   Ya te ense√±√© a elegir un modelo, VAE y escribir tus prompt, ahora podr√°s saber sobre todo el resto de las opciones disponibles antes de generar una imagen.

   Esperando traducci√≥n.
   
   ![Parameters](images/parameters.png)

   * **Sampling method:** These dictate how your image is formulated, and each produce different results. The default of `Euler a` is almost always the best. There are also very good results for `DPM++ 2M Karras` and `DPM++ SDE Karras`.
   * **Sampling steps:** These are "calculated" beforehand, and so more steps doesn't always mean more detail. I always go with 30, you may go from 20-50 and find good results.
   * **Width and Height:** 512x512 is the default, and you should almost never go above 768 in either direction as it may distort and deform your image. To produce bigger images see `Hires. fix`
   * **Batch Count and Batch Size:** Batch *size* is how many images your graphics card will generate at the same time, which is limited by its VRAM. Batch *count* is how many repetitions of those to produce. Batches have sequential seeds, more on seeds below.
   * **CFG Scale:** "Lower values produce more creative results". You should almost always stick to 7, but 4 to 10 is an acceptable range. It may get strange outside that.
   * **Seed:** A number that guides the creation of your image. The same seed with the same prompt and parameters produces almost exacly the same image every time.
  
   **Hires. fix:** Lets you create larger images without distortion. Often used at 2x scale. When selected, more options appear:
   * **Upscaler:** The algorithm to upscale with. `Latent` and its variations produce creative results, and you may also like `R-ESRGAN 4x+` and its anime version. I recommend the Remacri upscaler, see [Upscalers ‚ñº](#upscale). 
   * **Hires steps:** I recommend at least half as many as your sampling steps. Higher values aren't always better, and they take a long time, so be conservative here.
   * **Denoising strength:** The most important parameter. Near 0.0, no detail will be added to the image. Near 1.0, the image will be changed completely. I recommend something between 0.2 and 0.6 depending on the image, to add enough detail as the image gets larger, without *destroying* any original details you like.
    
   Others:
   * **Restore faces:** May improve realistic faces. I never need it with the models and prompts listed in this guide as well as hires fix.
   * **Tiling:** Used to produce repeating textures to put on a grid. Not very useful.
   * **Script:** Lets you access useful features and extensions, such as [X/Y/Z Plot ‚ñº](#plot) which lets you compare images with varying parameters on a grid. Very powerful.

&nbsp;
  
# Extensiones <a name="extensions"></a>[‚ñ≤](#index)

*Stable Diffusion WebUI* es el programa que estamos ocupando y √©ste permite a√±adir extensiones muy √∫tiles. Para ello dir√≠gete a la pesta√±a **Extensions** luego a **Install from URL**, y pega all√≠ estos enlaces de github. Luego presiona *Install* y espera que se instale. Finalmente ve a **Installed** y presiona *Apply and restart UI*.

Esperando traducci√≥n.
 
![Extensions](images/extensions.png)

Here are some useful extensions. Most of these come installed in the collab in this guide, and I hugely recommend you manually add the first 2 otherwise:
* [Image Browser (fixed fork)](https://github.com/aka7774/sd_images_browser) - This will let you browse your past generated images very efficiently, as well as directly sending their prompts and parameters back to txt2img, img2img, etc.
* [TagComplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete) - Absolutely essential for anime art. It will show you the matching booru tags as you type. Anime models work via booru tags, and rarely work at all if you go outside them, so knowing them is godmode. Not all tags will work well in all models though, specially if they're rare.
* [ControlNet](https://github.com/Mikubill/sd-webui-controlnet) - A huge extension deserving of [its own guide ‚ñº](#controlnet). It lets you take AI data from any image and use it as an input for your image. Practically speaking, it can create any pose or environment you want. Very powerful if used with external tools succh as Blender.
* [Ultimate Upscale](https://github.com/Coyote-A/ultimate-upscale-for-automatic1111) - A semi-advanced script usable from the img2img section to make really large images, where normally you can only go as high as your VRAM allows. See [Ultimate Upscaler ‚ñº](#ultimate).
* [Two-shot](https://github.com/opparco/stable-diffusion-webui-two-shot) - Normally you can't create more than one distinct character in the same image without them blending together. This extension lets you divide the image into parts; full, left side, right side; allowing you to make nice 2-character images. It is an optional launch setting in the collab.
* [Dynamic Prompts](https://github.com/adieyal/sd-dynamic-prompts) - A script to let you generate randomly chosen elements in your image, among other things.
* [Model Converter](https://github.com/Akegarasu/sd-webui-model-converter) - Lets you convert most 7GB/4GB models down to 2GB, by choosing `safetensors`, `fp16`, and `no-ema`. These pruned models work "almost the same" as the full models, which is to say, there is no appreciable difference due to math reasons. Most models come in 2 GB form nowadays regardless.

&nbsp;

# Loras <a name="lora"></a>[‚ñ≤](#index)

Los Loras son una tecnolog√≠a moderna y un tipo de **Extra Network** que permite a√±adir una especie de modelo peque√±o a cualquiera de tus modelos principales. Son similares a los embeddings, uno de los cuales te mostr√© [antes ‚ñ≤](#promptneg), pero los Loras son m√°s grandes y com√∫nmente m√°s capaces. No entrar√© en detalles t√©cnicos.

Un Lora puede representar un personaje, estilo, pose, ropa, o incluso un rostro humano (aunque no estoy de acuerdo con ello). Los checkpoints son bastante capaces para contenido general, pero para detalles como estos es donde comienzan a fallar y necesitar√°s un Lora. Podr√°s descargar Loras desde [civitai](https://civitai.com) u [otros lugares (NSFW)](https://gitgud.io/gayshit/makesomefuckingporn#lora-list) y su tama√±o es de 144 MB por defecto, pero pueden ser tan peque√±os como 1 MB. Los Loras m√°s grandes no son necesariamente mejores. Los Loras vienen en formato `.safetensors` de igual forma que los checkpoints.

Coloca tus archivos de Lora en la carpeta `stable-diffusion-webui/models/Lora`, o si est√°s usando el collab de esta gu√≠a pega el enlace directo a la descarga en la casilla `custom_urls`. Luego encuentra el bot√≥n üé¥ *Show extra networks* bajo el gran bot√≥n naranjo, el cual abrir√° una nueva secci√≥n de extra networks. Presiona la pesta√±a Lora y presiona **Refresh** para escanear nuevos Loras. Cuando hagas click en uno de tus Loras se a√±adir√° a tu prompt, y se ver√° as√≠: `<lora:archivo:1>` . Siempre se ver√°n as√≠, donde "archivo" es el nombre exacto del archivo en tu sistema (antes de `.safetensors`). Finalmente, el n√∫mero es el peso, lo cual expliqu√© [previamente ‚ñ≤](#promptweight). La mayor√≠a de Loras funcionan con un peso entre 0.5 y 1, y los valores muy grandes pueden "cocinar" tu imagen, especialmente si usas m√°s de uno al mismo tiempo.

![Extra Networks](images/extranetworks.png)

Adem√°s, muchos Loras tendr√°n una "palabra de activaci√≥n" para que tomen efecto, por ejemplo el nombre del personaje en caso de ser un Lora de personaje.

Un ejemplo de Lora es [Thicker Lines Anime Style](https://civitai.com/models/13910/thicker-lines-anime-style-lora-mix), un gran estilo de √°nime cl√°sico si deseas probarlo. No tiene palabra de activaci√≥n.

&nbsp;

# Im√°genes Grandes <a name="upscale"></a>[‚ñ≤](#index)

Esperando traducci√≥n.

As mentioned in [Generation Parameters ‚ñ≤](#parameters), normally you shouldn't go above 768 width or height when generating an image. Instead you should use `Hires. fix` with your choice of upscaler and an appropiate denoising level. Hires fix is limited by your VRAM however, so you may be interested in [Ultimate Upscaler ‚ñº](#ultimate) to go even larger.

You can download additional upscalers and put them in your `stable-diffusion-webui/models/ESRGAN` folder. They will then be available in Hires fix, Ultimate Upscaler, and Extras.

The collab in this guide comes with several of them, including **Remacri**, which is one of the best for all sorts of images.

* A few notable ones can be [found here](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/ESRGAN).
* LDSR is an advanced yet slow upscaler, its model and config can be [found here](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/LDSR) and both must be placed in `stable-diffusion-webui/models/LDSR`.
* The [Upscale Wiki](https://upscale.wiki/wiki/Model_Database) contains dozens of historical choices.

In the future I may present examples for each upscaler.

&nbsp;

# Scripts <a name="scripts"></a>[‚ñ≤](#index)

Esperando traducci√≥n.

Scripts can be found at the bottom of your generation parameters in txt2img or img2img.

* **X/Y/Z Plot** <a name="plot"></a>[‚ñ≤](#index)

   Capable of generating a series of images, usually with the exact same seed, but varying parameters of your choice. Can compare almost anything you want, including different models, parts of your prompt, sampler, upscaler and much more. You can have 1, 2, or 3 variable parameters, hence the X, Y and Z.

   Your parameters in X/Y/Z Plot are separated by commas, but anything else can go inbetween. The most common parameter to compare is **S/R Prompt**, where the first term is a phrase in your prompt and each term afterwards will replace the original. Knowing this, you can compare, say, Lora intensity, like this:

   `<lora:my lora:0.4>, <lora:my lora:0.6>, <lora:my lora:0.8>, <lora:my lora:1>`

   Here I made a comparison between different **models** (columns) and faces of different ethnicities via **S/R Prompt** (rows):

   <details>
   <summary>X/Y/Z Plot example, click to expand</summary>
   
   ![X Y Z plot of models and ethnicities](images/XYZplot.png)
   </details>

   **Tip:** It appears possible to do S/R with commas by using quotes like this (note no spaces between the commas and quotes): `"term 1, term 2","term 3, term 4","term 5, term 6"`

* **Prompt Matrix** <a name="matrix"></a>[‚ñ≤](#index)

   Similar conceptually to S/R from before, but more in-depth. It works by showing each possible combination of terms listed between the `|` symbol in your prompt, for example: `young man|tree|city` will always contain "young man", but we'll see what happens when we add or remove "tree" and "city". You can use commas and spaces just fine between the `|`.

   Inside the script, you will choose either your prompt or your negative prompt to make a matrix of, and whether the variable terms should be put at the start or the end.

   <a name="matrixneg"></a>Here is a comparison using the negative prompts I showed you in [Prompts ‚ñ≤](#prompt). We can see how EasyNegative affects the image, as well as how the rest of the prompt affects the image, then both together:

   <details>
   <summary>Prompt matrix examples, click to expand</summary>
  
   ![Prompt matrix of anime negative prompt sections](images/promptmatrix1.png)
   ![Prompt matrix of photorealistic negative prompt sections](images/promptmatrix2.png)
   </details>

* **Ultimate Upscale** <a name="ultimate"></a>[‚ñ≤](#index)

   An improved version of a builtin script, it can be added as an [extension ‚ñ≤] and used from within **img2img**. Its purpose is to resize an image and add more detail way past the normal limits of your VRAM by splitting it into chunks, although slower. Here are the steps:

   1. Generate your image normally up to 768 width and height, you can then apply hires fix if you are able to.

   1. From txt2img or the Image Browser extension send it directly into img2img, along with its prompt and parameters.

   1. Set the **Denoising** somewhere between 0.1 and 0.4. If you go higher you most likely will experience mutations.

   1. Go down to **Scripts** and choose **Ultimate SD Upscale**. Then, set your parameters like this, with your desired size and upscaler, and the **"Chess" Type**:
   
      ![Ultimate upscale parameters](images/ultimate.png)

      * If you have enough VRAM, you may increase the **Tile width** as well as the **Padding**. For example, doubling both of them. **Tile height** can remain at 0 and it'll match the width.
     
      * It is not necessary to set the **Seams fix** unless you encounter visible seams between regions in the final image.
     
   1. Generate your image and wait. You can watch the squares get sharper if you have image previews enabled.

&nbsp;

# ControlNet <a name="controlnet"></a>[‚ñ≤](#index)

ControlNet es una tecnolog√≠a reciente extremadamente poderosa. Te permite analizar una imagen para guiar la creaci√≥n de tus propias im√°genes con Stable Diffusion. Veremos qu√© significa esto en un momento.

Si est√°s usando el collab de esta gu√≠a activa la casilla de `all_control_models`. Sino, deber√°s instalar la [extension ControlNet ‚ñ≤](#extensions), luego ir [aqu√≠](https://civitai.com/models/9251/controlnet-pre-trained-models) y descargar modelos de controlnet que deber√°s poner en la carpeta `stable-diffusion-webui/extensions/sd-webui-controlnet/models`. Recomiendo los modelos Canny, Depth, Openpose y Scribble, los cuales veremos en un momento.

Voy a demostrar c√≥mo ControlNet puede ser usado. Para ello tomar√© una imagen popular en internet como nuestra "imagen de muestra". No es necesario que me sigas paso a paso, pero puedes descargar las im√°genes y ponerlas en la pesta√±a **PNG Info** para ver los datos de generaci√≥n.

Primero, debes estar en txt2img y bajar para presionar el men√∫ ControlNet. Una vez abierto presiona *Enable*, y elige un *preprocessor* y *model* con el mismo nombre. Para empezar elegir√© Canny para ambos. Finalmente a√±adir√© mi imagen de muestra. Aseg√∫rate de no clickear sobre la imagen de muestra o comenzar√°s a dibujar. Podemos ignorar el resto de las opciones.

![Control Net](images/controlnet.png)

* **Canny**

   El m√©todo Canny extrae los detalles de la imagen de muestra. Es √∫til para imitar todo tipo de im√°genes. Observa:

   <details>
   <summary>Ejemplo de Canny, click para expandir</summary>
   
   ![Canny preprocessed image](images/canny1.png)
   ![Canny output image](images/canny2.png)
   </details>

* **Depth**

   El m√©todo Depth extrae los elementos 3D de la imagen de muestra. Es de enorme utilidad cuando deseas imitar ambientes complejos y la composici√≥n general de una imagen. Observa:

   <details>
   <summary>Ejemplo de Depth, click para expandir</summary>
   
   ![Depth preprocessed image](images/depth1.png)
   ![Depth output image](images/depth2.png)
   </details>

* **Openpose**

   El m√©todo Openpose extrae las poses humanas de la imagen de muestra. Es de extrema utilidad para obtener la toma deseada y composici√≥n de uno de tus personajes. Observa:

   <details>
   <summary>Ejemplo de Openpose, click para expandir</summary>
   
   ![Open Pose preprocessed image](images/openpose1.png)
   ![Open Pose output image](images/openpose2.png)
   </details>

* **Scribble**

   Scribble te permite hacer un bosquejo y convertirlo en una pieza terminada con ayuda de tu prompt. Este es el √∫nico ejemplo de aqu√≠ que no comparte la misma imagen de muestra.

   <details>
   <summary>Ejemplo de Scribble, click para expandir</summary>
   
   ![Scribble sample image](images/scribble1.jpg)
   ![Scribble output image](images/scribble2.png)
   </details>

Podr√°s notar que hay 2 resultados para cada m√©todo. El primero es en paso intermedio llamado la "imagen pre-procesada", la cual se usa para producir la imagen final. Puedes entregar una imagen pre-procesada t√∫ mismo, en tal caso deber√°s elegir un preprocessor de *None*. Esto puede ser tremendamente poderoso tomando en cuenta herramientas externas tales como Blender y Photoshop.

En la pesta√±a Settings habr√° una secci√≥n ControlNet donde podr√°s activar *m√∫ltiples controlnets al mismo tiempo*. Un uso particularmente √∫til es cuando uno de ellos es Openpose, para obtener tanto la pose deseada como el ambiente deseado, o con la posici√≥n exacta de manos u otros detalles. Observa:

<details>
<summary>Ejemplo de Openpose+Canny, click para expandir</summary>
  
![Open Pose + Canny](images/openpose_canny.png)
</details>

Tambi√©n puedes usar ControlNet en img2img, en tal caso la imagen de entrada y la imagen de muestra ambas tendr√°n ciertos efectos en el resultado. No tengo mucha experiencia con este m√©todo.

Adem√°s, existen la version **diff** de los modelos de controlnet, los cuales producen resultados ligeramente distintos. Puedes [probarlos](https://civitai.com/models/9868/controlnet-pre-trained-difference-models) si deseas, pero yo no lo he hecho.

&nbsp;

# Entrenamiento de Loras para novatos <a name="train"></a>[‚ñ≤](#index)

Entrenar un [Lora ‚ñ≤](#lora) t√∫ mismo es una especie de logro. No es la gran cosa, pero hay muchas variables involucradas, y mucho trabajo dependiendo de las t√©cnicas que utilices. Es una mezcla entre un arte y una ciencia.

Puedes entranar Loras en tu propio computador si tienes al menos 8 GB de VRAM. Sin embargo utilizar√© un documento de Google Collab por motivos educacionales.

He aqu√≠ unos recursos cl√°sicos si deseas leer sobre el tema en profundidad. Puede que Rentry est√© bloqueado por tu proveedor de internet, en tal caso puedes usar un VPN o intentar poner la p√°gina a trav√©s de [Google Translate](https://translate.google.cl/?op=websites).
* [Entrenamiento de Loras, en Rentry](https://rentry.org/lora_train)
* [Ciencia de Loras, en Rentry](https://rentry.org/lora-training-science)
* [Entrenador original de Kohya (m√©todo Dreambooth)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)
* [Lista de par√°metros del entrenador](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts#list-of-arguments)

Con dichos recursos mucho m√°s inteligentes puestos de lado, intentar√© producir una gu√≠a simple para que puedas hacer tu propio Lora, de un personaje, concepto o estilo.

![Trainer collab](images/trainercollab.png)

1. Utilizaremos [ESTE COLLAB](https://colab.research.google.com/drive/1WVTkW0IOeiBrs6s79XuJ9r1u42fKw81L?usp=sharing). Puedes copiarlo a tu Google Drive si deseas.

1. Presiona el bot√≥n de reproducci√≥n de *A: Montar tu google drive* y dale acceso cuando lo pida. Haz lo mismo con *B: Instalaci√≥n*. Mientras se instala, sigue al siguiente paso.

1. Baja a *C: Configuraci√≥n* pero a√∫n no lo actives. Aqu√≠ en **Inicio** puedes darle cualquier nombre a tu proyecto. Tambi√©n puedes cambiar el modelo base que utilizaremos, pero para esta gu√≠a utilizaremos AnythingV3_fp16 ya que es la base de todos los modelos anime y produce los mejores resultados para ello. Si deseas entrenar con fotograf√≠as puedes copiar el enlace al modelo base de [Stable Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) o al modelo realista que desees utilizar (tal como [Deliberate](https://civitai.com/api/download/models/15236)). Recuerda tambi√©n cambiar el `model_type` a safetensors en tal caso.

1. **Archivos de entrenamiento** <a name="datasets"></a>[‚ñ≤](#index)
  
   Esta es la mayor parte del entrenamiento de Loras. Necesitar√°s recopilar un "dataset" o archivos de entrenamiento, los cuales consisten en im√°genes y sus correspondientes descripciones (con tags en el caso de anime).

   1. Encuentra im√°genes online que representes el personaje/concepto/estilo que deseas entrenar, posiblemente en sitios tales como [safebooru](https://safebooru.org/), [gelbooru](https://gelbooru.com/) o [danbooru](https://danbooru.donmai.us/). Necesitas al menos 10 im√°genes, idealmente 20 o m√°s, pero puedes usar cientos si deseas.
  
   1. Puedes crear los tags t√∫ mismo, lo cual es lento y poco preciso. Opcionalmente puedes agregar la [extensi√≥n Tagger](https://github.com/toriato/stable-diffusion-webui-wd14-tagger) a tu programa, la cual analiza todas tus im√°genes de entranemiento y genera tags para ellas.
  
   1. Opcionalmente puedes agregar otra extensi√≥n llamada [Tag Editor](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor) la cual te permite editar los tags de todos tus archivos al mismo tiempo.
  
   1. Una vez que tus im√°genes y descripciones est√©n listos, ponlos en una carpeta con la siguiente estructura: Una carpeta con el nombre de tu proyecto, la cual contiene al menos 1 carpeta en el formato `repeticiones_nombre`, la cual contiene tus archivos de entrenamiento. As√≠:
  
      ![Estructura de carpetas](https://i.imgur.com/zpbs8FB.png)

   1. Decide tu n√∫mero de repeticiones. Asumiendo que tienes alrededor de 20 im√°genes, recomiendo al menos 10 repeticiones. En tal caso, tu carpeta interior se llamar√° `10_minuevolora` o algo similar.
  
   1. Sube la carpeta exterior y todos sus contenidos (la que tiene el nombre de tu proyecto) a tu Google Drive, en la carpeta `lora_training/datasets/`.
  
1. **Opciones de entrenamiento** <a name="trainparams"></a>[‚ñ≤](#index)

   * Bajo **Archivos**, no necesitas cambiar nada.
   * Bajo **Pasos de Entrenamiento**, sigue las instrucciones para calcular tus pasos totales (`max_train_steps`). Recomiendo al menos 400 pasos totales, lo cual deber√≠a tomar 15 o 20 minutos. Puedes editar `lr_warmup_steps` para que sea igual a tu cantidad de im√°genes.
   * Bajo **Opciones de Entrenamiento**, el `unet_lr` or "learning rate" (velocidad de aprendizaje) es el par√°metro m√°s importanto. 1e-3 es el valor por defecto y funciona cuando tienes pocas im√°genes, pero puede ir hasta 1e-5.
   * Nota sobre `network_dim`: El dim es el tama√±o de tu Lora. La mayor√≠a de personas entrena Loras con dim 128, los cuales pesan 144 MB, y es totalmente innecesario. Recomiendo un dim de 16 en la mayor√≠a de casos. Puedes incluso bajar a 1 y a√∫n obtener resultados decentes.

1. Ahora puedes activar *C: Configuraci√≥n*, esperar que el modelo se descarge, y finalmente comenzar el entrenamiento con *D: Cocinar el Lora*. Esperemos que todo salga bien. Sino, intenta contactarme o buscar ayuda para resolver errores.

1. **Probar tus resultados** <a name="traintest"></a>[‚ñ≤](#index)

   Ha pasado un rato y tu Lora termin√≥ de entrenar/cocinar. Ve y desc√°rgalo de la carpeta `lora_training/output` en tu google drive. Pero ver√°s que hay m√°s de uno; por defecto, se guarda una copia de tu Lora cada 2 epochs, permiti√©ndote as√≠ comparar su progreso. Si entrenas tu Lora por muchos epochs, podr√°s identificar el punto √≥ptimo entre que est√© "crudo" o "recocido".

   Cuando un Lora est√° "crudo", no alcanzar√° a imitar tus datos de entrenamiento. Cuando est√° "recocido", imita tus datos de entrenamiento *demasiado*, lo cual evita que pueda hacer cualquier otra cosa. Y si no a√±adiste suficientes datos o datos de baja calidad, ¬°puede que est√© crudo y recocido al mismo tiempo!

   Usando lo aprendido en [X/Y/Z Plot ‚ñ≤](#plot), podemos hacer una comparaci√≥n del progreso de nuestro Lora:

   ![Comparaci√≥n del resultado de Lora](images/loratrain.png)

   Mira eso, ¬°se vuelve cada vez m√°s detallado! La √∫ltima imagen no tiene ning√∫n Lora para comparar. Este parece ser un Lora de personaje exitoso, pero necesitar√≠amos probar una variedad de semillas, prompts y escenas para estar seguros.

   Es com√∫n que tu Lora "queme" o distorsione tus im√°genes al ser usado con pesos altos como 1, sobre todo si est√° recocido. Un peso entre 0.5 y 0.8 es aceptable para nosotros. Puede que necesites ajustar la velocidad de aprendizaje o el dim para esto, u otras variables no encontradas en este collab. Si est√°s leyendo esto y conoces los secretos de los Lora, h√°znoslo saber.

   Despu√©s de acostumbrarse a hacer Loras, e interactuar on la comunidad y sus variados recursos, estar√°s listo para usar otro m√©todo m√°s avanzado como el [collab original todo-en-uno de kohya](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Buena suerte.

* **Consejos adicionales** <a name="trainchars"></a>[‚ñ≤](#index)

   La parte m√°s importante para un personaje son los tags. Claro que necesitas im√°genes con variadas poses y lugares, pero si las descripciones est√°n mal no servir√° de nada.

   Cuando entrenas un personaje o concepto deber√≠as definir una **palabra de activaci√≥n**, y ajustar el valor de `keep_tokens` a 1. Una palabra de activaci√≥n es como podremos invocar a tu Lora para que funcione. Habiendo hecho eso, quieres quitar o "limpiar" las tags que son intr√≠nsicas a tu personaje o concepto, tales como el color de pelo y ojos. Por ejemplo, si una chica siempre tiene orejas de gato, quieres quitar las tags tales como `animal ears, animal ear fluff, cat ears`, y as√≠ √©stas ser√°n "absorbidas" por tu palabra de activaci√≥n.

   Tambi√©n puedes limpiar las tags de atuendo, dejando as√≠ s√≥lo los aspectos m√°s relevantes de la ropa y eliminando las redundancias, por ejemplo dejar "tie" pero quitar "red tie". Esto facilitar√° que estas ropas absorban los detalles relevantes. Incluso puedes definir una palabra de activaci√≥n para cada atuendo importante, por ejemplo personaje-normal, personaje-bikini, etc. Pero hay m√°s de una manera de lograr esto. En cualquier caso, con el uso correcto de tags, tu personaje deber√≠a ser capaz de cambiar de ropa f√°cilmente.

   Mientras tanto, los Loras de estilo no necesitan palabra de activaci√≥n, ya que deseamos que siempre est√©n activos. Absorber√°n el estilo art√≠stico de forma natural, y funcionar√° con variados pesos.

   Esta "absorci√≥n" de detalles no entregados por los tags es la forma en que los Loras funcionan en general, ya que logran aprender y representar los detalles imperceptibles o dif√≠ciles de explicar tales como el rostro, acccesorios, composici√≥n, etc.

&nbsp;

# ...vtubers? <a name="vtubers"></a>[‚ñ≤](#index)

Y as√≠ llegamos la final de la gu√≠a. Gracias por leer. Si tienes correcciones o contribuciones puedes abrir un Issue o un Pull Request en esta p√°gina y echar√© un vistazo pronto.

Tengo [otra p√°gina dedicada a Loras de vtubers, en especial Hololive](https://huggingface.co/hollowstrawberry/holotard). Si es que es de tu inter√©s.

Saludos.

&nbsp;
# √çndice <a name="index"></a>

* [Introducci√≥n](#intro)
* [Google Collab](#collab)
* [Instalaci√≥n Local (Windows + Nvidia)](#install)
* [Conceptos Esenciales](#start)
   1. [Modelos](#model)
   1. [VAEs](#vae)
   1. [Prompts](#prompt)
   1. [Opciones de generaci√≥n](#gen)
* [Extensiones](#extensions)
* [Loras](#lora)
* [Im√°genes Grandes](#upscale)
* [Scripts](#imgscripts)
   * [X/Y/Z Plot](#plot)
   * [Prompt Matrix](#matrix)
   * [Ultimate Upscaler](#ultimate)
* [ControlNet](#controlnet)
* [Entrenamiento de Loras para novatos](#train)
    * [Archivos de entrenamiento](#datasets)
    * [Opciones de entrenamiento](#trainparams)
    * [Probar tus resultados](#traintest)
    * [Consejos adicionales](#trainchars)
* [...vtubers?](#vtubers)
 
&nbsp;

# Introducci√≥n <a name="intro"></a>[‚ñ≤](#index)

Stable Diffusion es una poderosa herramienta de generaci√≥n de im√°genes a trav√©s de inteligencia artificial (IA), la cual puedes usar en tu propio hogar. √âsta utiliza "modelos", los cuales son el cerebro de la IA y pueden crear casi cualquier cosa, siempre y cuando alguien los haya entrenado para ello. Los usos m√°s populares son generaci√≥n de arte anime, de fotorealismo, y de contenido para adultos.

Las im√°genes que creas pueden ser usadas para cualquier prop√≥sito, siempre y cuando sigan la licencia del modelo utilizado. Estas im√°genes puede o no que sean "tuyas" en un sentido legal, dependiendo de las leyes de tu pa√≠s, y com√∫nmente es inconcluso. Ni yo ni nadie asociado con Stable Diffusion y sus modelos somos responsables por el contenido que generes, y se te prohibe usar estas herramientas para generar contenido ilegal o da√±ino.

Esta gu√≠a est√° actualizada hasta Marzo de 2023. Una semana es como un a√±o para el desarrollo de IAs, as√≠ que espero que siga siendo √∫til para cuando la leas.

&nbsp;

# Google Collab <a name="collab"></a>[‚ñ≤](#index)

La manera m√°s facil de usar Stable Diffusion es a trav√©s de Google Collab. Con √©l tomas prestado los computadores de Google para usar la IA, con tiempo limitado, com√∫nmente varias horas al d√≠a. Necesitar√°s al menos una cuenta de Google y utilizaremos el Google Drive para guardar tus im√°genes.

Si en su lugar deseas correr el programa en tu propio computador, [baja aqu√≠ ‚ñº](#install).

Aqu√≠ las instrucciones del collab. Estaremos usando el stable-diffusion-webui de Automatic1111.

1. Abre [esta p√°gina](https://colab.research.google.com/drive/1wEa-tS10h4LlDykd87TF5zzpXIIQoCmq).

1. Cerca de arriba clickea **Copiar a mi Drive**. Espera que se abra la ventana nueva y cierra la vieja. Ahora tienes tu propio collab el cual puedes configurar a tu gusto, y deber√°s abrir desde tu Google Drive. En caso de actualizaciones deber√°s ver el original.

1. Activa las siguientes casillas bajo **Configurations**: `output_to_drive, configs_in_drive, no_custom_theme`. Luego, activa las siguientes casillas bajo **Models, etc**: `anything_vae`, `wd_vae`, `sd_vae`.

1. Si ya conoces Stable Diffusion puedes pegar los enlaces a tus recursos deseados en la casilla de `custom_urls`. Vamos a a√±adir enlaces aqu√≠ m√°s adelante en la gu√≠a. Los enlaces deben ser **descargas directas** a cada archivo (idealmente de los sitios civitai o huggingface), y deben separarse por comas.

1. Presiona el bot√≥n de reproducci√≥n a la izquierda, en cualquier lugar dentro de la gran secci√≥n llamada **Start üöÄ**. Espera un par de minutos para que se instale y corra el programa. Ver√°s aparecer mensajes de progreso m√°s abajo. Eventualmente uno de estos mensajes ser√° un **public link** lo cual indica que est√° listo, y puedes abrir este enlace en una nueva pesta√±a para utilizar Stable Diffusion. **Mant√©n la pesta√±a del collab abierta!** (esto puede ser dif√≠cil si intentas usarlo desde un tel√©fono)

1. Ahora puedes hacer algunas im√°genes decentes gracias al modelo por defecto llamado **Anything 4.5**. Pero podemos hacer m√°s que ello, y adem√°s, ¬øqu√© son todas estas opciones? [Baja aqu√≠ ‚ñº](#start) para aprender las bases.

&nbsp;

# Instalaci√≥n Local (Windows + Nvidia) <a name="install"></a>[‚ñ≤](#index)

Para correr Stable Diffusion en tu propio computador necesitar√°s al menos 16 GB de RAM y 4 GB de VRAM (idealmente 8). Por ahora s√≥lo voy a explicar el caso en que uses Windows 10/11 y poseas una tarjeta gr√°fica NVIDIA de serie 10XX o mayor. Mis disculpas si tienes AMD o usas Linux o Mac, pero con ellos es m√°s complejo. Si no cumples estas condiciones a√∫n puedes usar el Google Collab [aqu√≠ arriba ‚ñ≤](#collab).

Aqu√≠ las instrucciones de instalaci√≥n. Utilizaremos un launcher para correr el stable-diffusion-webui de Automatic1111.

1. Obt√©n el instalador m√°s reciente desde [esta p√°gina](https://github.com/EmpireMediaScience/A1111-Web-UI-Installer/releases). 

1. Corre el instalador (dile a Windows que no es un virus) y selecciona una ubicaci√≥n sencilla y accesible en donde instalar. Espera a que termine.

1. Corre el programa, √©ste es el launcher. Ver√°s algunas opciones. Primero activa **medvram** y **xformers**. Si tu gr√°fica tiene 12 GB de VRAM o m√°s no es necesario medvram.

1. En la casilla de texto que dice *Additional Launch Options* pega lo siguiente: `--opt-channelslast --no-half-vae --theme dark` . Si a√±ades m√°s opciones sep√°ralas con espacios.
   * Si tu gr√°fica tiene 4 o 6 GB de VRAM a√±ade `--opt-split-attention-v1` lo cual puede ayudar un poco.
   * Si deseas correr el programa en un dispositivo y usarlo a trav√©s de otro dispositivo en la misma red de WiFi (como en tu tel√©fono), puedes a√±adir `--listen --enable-insecure-extension-access` . Tras iniciar el programa podr√°s conectarte desde el navegador usando la IP local en el puerto 7860. Tambi√©n puedes a√±adir una contrase√±a con `--gradio-auth nombre:contr` .
   * Puedes encontrar todas las opciones [aqu√≠](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings).

1. Presiona **Launch** y espera a que cargue y/o termine de instalar. Cuando termine se abrir√° una ventana en tu navegador.

1. La p√°gina est√° abierta, es tu propio sitio web privado. Aqu√≠ en la pesta√±a principal (txt2img) es donde har√°s casi todas tus im√°genes. Pero primero iremos a la pesta√±a **Settings**, y veremos algunas secciones del lado izquierdo.
   * En la secci√≥n de *Stable Diffusion* baja al final y aumenta el **Clip skip** de 1 a 2. Se dice que produce mejores im√°genes.
   * En la secci√≥n *User Interface*, baja hasta **Quicksettings list** y c√°mbialo a `sd_model_checkpoint, sd_vae` .
   * Vuelve a subir y presiona el gran **Apply settings**, luego **Reload UI**.

1. Ahora est√°s m√°s que listo para generar im√°genes, pero s√≥lo tienes el modelo b√°sico. No es muy bueno, sirve para pinturas entre otras cosas. Adem√°s, ¬øqu√© son todas estas opciones? Ve [aqu√≠ abajo ‚ñº](#start) para aprender las bases.

&nbsp;

# Conceptos Esenciales <a name="start"></a>[‚ñ≤](#index)

Antes o despu√©s de hacer tus primeras im√°genes, querr√°s leer la informaci√≥n de aqu√≠ abajo para mejorar tu experiencia y resultados. Si seguiste las instrucciones de esta gu√≠a, la parte de arriba de tu p√°gina de *Stable Diffusion WebUI* deber√≠a verse parecida a esto:

![Arriba](images/top.png)

Aqu√≠ puedes seleccionar un checkpoint y un VAE. Ahora explicar√© qu√© son ambas cosas y c√≥mo obtenerlas. El collab tiene m√°s opciones aqu√≠ arriba pero puedes ignorarlas.

1. **Modelos** <a name="model"></a>[‚ñ≤](#index)

   El **modelo**, tambi√©n llamado **checkpoint**, es el cerebro de tu IA, dise√±ado para producir cierto tipo de im√°genes. Hay muchas opciones, las cuales puedes encontrar aqu√≠ en huggingface o en [civitai](https://civitai.com). Ya que a√∫n no sabes elegir, estas son mis recomendaciones:
   * Para hacer anime, [7th Heaven Mix](https://civitai.com/models/4669/corneos-7th-heaven-mix) tiene un estilo placentero, est√©ticamente parecido a las pel√≠culas de anime, mientras que [Abyss Orange Mix 3](https://civitai.com/models/9942/abyssorangemix3-aom3) *(__Nota:__ Baja all√≠ y elige la opci√≥n AOM3)* ofrece m√°s realismo con luces suaves, y m√°s lascivia. Personalmente mezcl√© estas dos opciones creando as√≠ [Heaven Orange Mix](https://civitai.com/models/14305/heavenorangemix).
   * Aunque AOM3 es extremadamente capaz de hacer contenido para adultos, el popular modelo de hentai [Grapefruit](https://civitai.com/models/2583/grapefruit-hentai-model) tambi√©n puede cumplir tus deseos.
   * Para arte en general elige [DreamShaper](https://civitai.com/models/4384/dreamshaper), no hay nada que se le acerque en t√©rminos de creatividad. Tambi√©n est√° [Pastel Mix](https://civitai.com/models/5414/pastel-mix-stylized-anime-model), el cual tiene una hermosa y √∫nica est√©tica con un poco de anime.
   * Para el fotorealismo recomiendo [Deliberate](https://civitai.com/models/4823/deliberate). Puede hacer casi cualquier cosa, pero fotos en especial. Muy detallado.
   * El modelo [URPM](https://civitai.com/models/2661/uber-realistic-porn-merge-urpm) es la mayor concentraci√≥n de pornograf√≠a que vas a encontrar.
   
   Si est√°s usando el collab de est√° gu√≠a, copia el **enlace directo a la descarga** y p√©galo en la casilla llamada `custom_urls`. Separa m√∫ltiples enlaces usando comas.

   En una instalaci√≥n local, com√∫nmente los modelos deben ir dentro de la carpeta `stable-diffusion-webui/models/Stable-diffusion`.

   Una nota importante es que los checkpoints deben estar en formato `.safetensors`, ya que algunos archivos `.ckpt` **pueden** contener virus. Ten cuidado. Adem√°s, cuando elijas modelos a veces ver√°s varias opciones tales comoo fp32, fp16 y pruned. Para generar im√°genes todas funcionan igual, as√≠ que elige el archivo m√°s peque√±o (pruned-fp16). Para mezclar o entrenar modelos se recomienda el archivo m√°s grande.

   **Consejo:** Tras a√±adir el archivo de un recurso a las carpetas del programa, podr√°s encontrarlo tras presionar üîÉ junto al lugar donde lo selecciones.

1. **VAEs** <a name="vae"></a>[‚ñ≤](#index)

   La mayor√≠a de checkpoints no viene con VAE inclu√≠do. El VAE es un peque√±o modelo aparte, el cual "convierte tu imagen a formato humano". Sin un VAE tus im√°genes van a tener malos colores y detalles.

   Si usas el collab de esta gu√≠a, te hice elegir todos los VAEs antes de iniciar el programa.
   
   Pr√°cticamente s√≥lo hay 3 VAEs en circulamiento:
   * [anything vae](https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt), tambi√©n conocido como orangemix vae. Todos los modelos de anime lo utilizan.
   * [vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors), el oficial de Stable Diffusion, utilizado com√∫nmente por modelos realistas.
   * [kl-f8-anime2](https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt), tambi√©n conocido como el vae de Waifu Diffusion, es m√°s viejo y tiene colores m√°s brillantes. Lo utiliza Pastel Mix.

   Si usas el launcher puedes elegir el VAE antes de iniciar el programa, de otra forma los VAEs deben ir en la carpeta `stable-diffusion-webui/models/VAE`.
   
   Si no has seguido esta gu√≠a hasta este punto, dentro de tu p√°gina deber√°s ir a la pesta√±a **Settings**, luego la seccci√≥n **Stable Difussion**, y all√≠ escoger tu VAE y guardar los cambios.

   **Consejo:** Tras a√±adir el archivo de un recurso a las carpetas del programa, podr√°s encontrarlo tras presionar üîÉ junto al lugar donde lo selecciones.

1. **Prompts** <a name="prompt"></a>[‚ñ≤](#index)

   Har√°s casi todas tus im√°genes en la primera pesta√±a de tu p√°gina, **txt2img**. Aqu√≠ ver√°s dos grandes casillas que llamamos *prompt* y *prompt negativo*. Aqu√≠ deber√°s describir lo que deseas que aparezca y no aparezca en tu imagen, **y debe ser en ingl√©s**.  
   Stable Diffusion no es como Midjourney u otros servicios populares, no puedes solo decir lo que quieres, sino que debes ser *muy* espec√≠fico.  
   Debido a esto la mayor√≠a de personas se aferran a alg√∫n prompt que le funcione bien a ellos, muchas veces recomendado por otra persona. Aqu√≠ recomiendo mis propios prompts y prompts negativos:
   
   * Anime
      * `2d, masterpiece, best quality, anime, highly detailed face, highly detailed eyes, highly detailed background, perfect lighting`
      * `EasyNegative, worst quality, low quality, 3d, realistic, photorealistic, (loli, child, teen, baby face), zombie, animal, multiple views, text, watermark, signature, artist name, artist logo, censored`
     
   * Fotorealismo
      * `best quality, 4k, 8k, ultra highres, (realistic, photorealistic, RAW photo:1.4), (hdr, sharp focus:1.2), intricate texture, skin imperfections`
      * `EasyNegative, worst quality, low quality, normal quality, child, painting, drawing, sketch, cartoon, anime, render, 3d, blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art`

   * **EasyNegative:** <a name="promptneg"></a>El prompt negativo que recomiendo utiliza EasyNegative, un *embedding* o "palabra m√°gica" que codifica muchas cosas malas para as√≠ mejorar tus im√°genes. De otra forma tu prompt negativo ser√≠a enorme.
      * Si usas el collab de esta gu√≠a, ya tienes instalado EasyNegative. Sino, es un archivo diminuto que puedes [descargar aqu√≠](https://huggingface.co/datasets/gsdf/EasyNegative/resolve/main/EasyNegative.safetensors) y debes colocar en la carpeta `stable-diffusion-webui/embeddings`. Finalmente debes reiniciar el programa para as√≠ poder usar esta palabra m√°gica.

   Puedes ver una comparaci√≥n de prompts negativos incluyendo EasyNegative m√°s abajo en [Prompt Matrix ‚ñº](#matrixneg).

   ![Prompts](images/prompt.png)

   Despu√©s de un "prompt base" como los que te he mostrado, puedes comenzar a escribir lo que desees. Por ejemplo, `young woman in a bikini in the beach, full body shot`. Tambi√©n puedes a√±adir m√°s t√©rminos negativos, como `old, ugly, futanari, furry`, etc.  
   Puedes guardar tus prompts usando los botones debajo de Generate. Presiona el peque√±o üíæ *Save style* y asigna un nombre al prompt actual. Tras ello podr√°s abrir tus *Styles* para elegirlo, y luego presionar üìã *Apply selected styles to the current prompt* para a√±adirlo.

   <a name="promptweight"></a>Debes saber que cuando encierras algo en `(par√©ntesis)`, tendr√° m√°s **peso** o √©nfasis/intensidad en tu imagen, lo cual equivale al valor de `1.1`. El valor base para todas las palabras es 1, y cada par√©ntesis multiplica por 1.1 nuevamente. Tambi√©n puedes especificar el peso t√∫ mismo, por ejemplo: `(full body:1.4)`. Tambi√©n puedes ir menor a 1 para quitar √©nfasis; los `[corchetes]` multiplican por 0.9, pero si quieres ser preciso tambi√©n necesitas par√©ntssis, como `(as√≠:0.5)`.

   Podr√°s notar que la IA es famosamente mala para hacer manos y pies. Con estos buenos prompt mejorar√°n un poco, pero quiz√° debas usar photoshop, inpainting, o t√©cnicas avanzadas como [ControlNet ‚ñº](#controlnet) para perfeccionar tu imagen.

1. **Opciones de generaci√≥n** <a name="gen"></a>[‚ñ≤](#index)

   Ya te ense√±√© a elegir un modelo, VAE y escribir tus prompt, ahora podr√°s saber sobre todo el resto de las opciones disponibles antes de generar una imagen.
   
   ![Parameters](images/parameters.png)

   * **Sampling method:** Es el algoritmo que genera tu imagen, cada uno con resultados distintos. El por defecto de `Euler a` casi siempre es el mejor, y tambi√©n obtendr√°s muy buenos resultados con`DPM++ 2M Karras` y `DPM++ SDE Karras`.
   * **Sampling steps:** La cantidad de pasos, son "calculados" con anticipacic√≥n y por lo tanto m√°s pasos no siempre es mejor. Yo siempre uso 30 pasos, pero de 20 a 50 encontrar√°s resultados consistentemente buenos.
   * **Width and Height:** La resoluci√≥n de 512x512 es lo normal. Si superas el ancho o alto de 768 tu imagen puede ser distorsionada y deformada. Para producir im√°genes m√°s grandes est√° la opci√≥n `Hires fix`.
   * **Batch Count and Batch Size:** El *size* es cu√°ntas im√°genes tu tarjeta gr√°fica producir√° al mismo tiempo, lo cual se limita for su VRAM. El *count* son las repeticiones del valor anterior. Los batches tienen seeds consecutivas, m√°s abajo ver√°s las seeds.
   * **CFG Scale:** "Los valores menores producen resultados m√°s creativos". Casi siempre debes dejarlo en 7, pero de 4 a 10 es un rango aceptable.
   * **Seed:** Un n√∫mero que dicta la generaci√≥n de tu imagen. La misma seed con el mismo prompt y opciones siempre producir√° la misma imagen, salvo detalles menores y algunas excepciones.
  
   **Hires fix:** Esta opci√≥n te permite crear im√°genes m√°s grandes sin problemas. Por lo general se ocupa para duplicar el ancho y alto. Cuando la actives, aparecer√°n m√°s opciones:
   * **Upscaler:** El algoritmo para agrandar la imagen. Se dice que `Latent` crea resultados creativos. Puede que tambi√©n te guste `R-ESRGAN 4x+` y su variante para anime. Recomiendo el upscaler llamado Remacri, del cual hablo [m√°s abajo ‚ñº](#upscale).
   * **Hires steps:** Recomiendo al menos la mitad de tus pasos normales. M√°s pasos no siempre es mejor, y son bastante lentos, as√≠ que s√© conservador.
   * **Denoising strength:** El valor m√°s importante. Cera de 0.0 tu imagen no tendr√° ning√∫n detalle nuevo. Cerca de 1.0, tu imagen cambiar√° completamente. Recomiendo un valor entre 0.2 y 0.6 dependiendo del caso, lo cual a√±ade suficiente detalle sin *destruir* los detalles existentes que te gusten.
    
   Others:
   * **Restore faces:** Puede mejorar los rostros reales. Nunca lo he necesitado con los prompt de esta gu√≠a y con hires fix.
   * **Tiling:** Sirve para hacer patrones repetitivos como baldosas, no es muy √∫til.
   * **Script:** Te permite acceder a funciones y extensiones muy √∫tiles, [las cuales explico m√°s abajo ‚ñº](#plot). Por ejemplo, X/Y/Z Plot permite comparar una cuadr√≠cula de im√°genes con diferentes opciones. Muy poderoso.

&nbsp;
  
# Extensiones <a name="extensions"></a>[‚ñ≤](#index)

*Stable Diffusion WebUI* es el programa que estamos ocupando y √©ste permite a√±adir extensiones muy √∫tiles. Para ello dir√≠gete a la pesta√±a **Extensions**, luego a **Install from URL**, y pega all√≠ alguno de estos enlaces de github. Luego presiona *Install* y espera que se instale. Cuando termines ve a **Installed** y presiona *Apply and restart UI*.
 
![Extensiones](images/extensions.png)

Aqu√≠ hay algunas extensiones √∫tiles. Si usas el collab de esta gu√≠a la mayor√≠a ya est√°n instaladas, sino, recomiendo enormemente instalar manualmente las primeras 2.
* [Image Browser (bugfix)](https://github.com/aka7774/sd_images_browser) - Navegador de Im√°genes, permite ver todas las im√°genes wue has creado y r√°pidamente enviarlas con sus par√°metros a txt2img, img2img, etc.
* [TagComplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete) - Completamente esencial para hacer anime, te muestra las tags de booru existentes mientras escribes tu prompt. Los modelos de anime funcionan a trav√©s de estos tags, haciendo de √©sta una de las mejores extensiones. Ojo que no todas las tags funcionan siempre, sobre todo si son poco comunes.
* [ControlNet](https://github.com/Mikubill/sd-webui-controlnet) - Enorme extensi√≥n con [su propia gu√≠a ‚ñº](#controlnet). Te permite analizar cualquier imagen existente y usarla como muestra para guiar tus propias im√°genes. En t√©rminos pr√°cticos, te permite replicar cualquier pose o ambiente que desees.
* [Ultimate Upscale](https://github.com/Coyote-A/ultimate-upscale-for-automatic1111) - Un script usable desde img2img que permite hacer im√°genes enormes aunque tengas poca vram, dividi√©ndolas en secciones aunque sea m√°s lento. [Ver su gu√≠a aqu√≠ ‚ñº](#ultimate).
* [Two-shot](https://github.com/opparco/stable-diffusion-webui-two-shot) - Normalmente no es posible crear escenas de dos personajes, ya que el prompt hace que se fusionen sus caracter√≠sticas. Esta extensi√≥n permite dividir la imagen en: todo, izquierda, derecha; permitiendo as√≠ tener escenas naturales con 2 personajes o temas al mismo tiempo.
* [Dynamic Prompts](https://github.com/adieyal/sd-dynamic-prompts) - Un script para tener prompts semi-aleatorios. Un poco complejo.
* [Model Converter](https://github.com/Akegarasu/sd-webui-model-converter) - Permite convertir modelos de 7 GB o 4 GB a 2 GB, seleccionando  `safetensors`, `fp16`, y `no-ema`. Estos modelos "pruneados" funcionan pr√°cticamente igual para generar im√°genes. La mayor√≠a de modelos hoy en d√≠a vienen en este formato de todas formas.

&nbsp;

# Loras <a name="lora"></a>[‚ñ≤](#index)

Los Loras son una tecnolog√≠a moderna y un tipo de **Extra Network** que permite a√±adir una especie de modelo peque√±o a cualquiera de tus modelos principales. Son similares a los embeddings, uno de los cuales te mostr√© [antes ‚ñ≤](#promptneg), pero los Loras son m√°s grandes y com√∫nmente m√°s capaces. No entrar√© en detalles t√©cnicos.

Un Lora puede representar un personaje, estilo, pose, ropa, o incluso un rostro humano (aunque no estoy de acuerdo con ello). Los checkpoints son bastante capaces para contenido general, pero para detalles como estos es donde comienzan a fallar y necesitar√°s un Lora. Podr√°s descargar Loras desde [civitai](https://civitai.com) u [otros lugares (NSFW)](https://gitgud.io/gayshit/makesomefuckingporn#lora-list) y su tama√±o es de 144 MB por defecto, pero pueden ser tan peque√±os como 1 MB. Los Loras m√°s grandes no son necesariamente mejores. Los Loras vienen en formato `.safetensors` de igual forma que los checkpoints.

Coloca tus archivos de Lora en la carpeta `stable-diffusion-webui/models/Lora`, o si est√°s usando el collab de esta gu√≠a pega el enlace directo a la descarga en la casilla `custom_urls`. Luego encuentra el bot√≥n üé¥ *Show extra networks* bajo el gran bot√≥n naranjo, el cual abrir√° una nueva secci√≥n de extra networks. Presiona la pesta√±a Lora y presiona **Refresh** para escanear nuevos Loras. Cuando hagas click en uno de tus Loras se a√±adir√° a tu prompt, y se ver√° as√≠: `<lora:archivo:1>` . Siempre se ver√°n as√≠, donde "archivo" es el nombre exacto del archivo en tu sistema (antes de `.safetensors`). Finalmente, el n√∫mero es el peso, lo cual expliqu√© [previamente ‚ñ≤](#promptweight). La mayor√≠a de Loras funcionan con un peso entre 0.5 y 1, y los valores muy grandes pueden "cocinar" tu imagen, especialmente si usas m√°s de uno al mismo tiempo.

![Extra Networks](images/extranetworks.png)

Adem√°s, muchos Loras tendr√°n una "palabra de activaci√≥n" para que tomen efecto, por ejemplo el nombre del personaje en caso de ser un Lora de personaje.

Un ejemplo de Lora es [Thicker Lines Anime Style](https://civitai.com/models/13910/thicker-lines-anime-style-lora-mix), un gran estilo de √°nime cl√°sico si deseas probarlo. No tiene palabra de activaci√≥n.

&nbsp;

# Im√°genes Grandes <a name="upscale"></a>[‚ñ≤](#index)

Como [mencionamos anteriormente ‚ñ≤](#parameters), normalmente no debes generar im√°genes sobre 768 de ancho y alto. Debes usar Hires fix, con un "upscaler" (algoritmo) y denoising (intensidad) apropiados. Hires fix est√° limitado por tu VRAM, por lo que te puede interesar [Ultimate Upscaler ‚ñº](#ultimate).

Es posible descargar upscalers adicionales y ponerlos en tu carpeta `stable-diffusion-webui/models/ESRGAN`. As√≠ funcionar√°n con Hires fix, Ultimate Upscaler, y Extras.

El collab de esta gu√≠a viene con varios de estos, incluyendo **Remacri**, uno de los mejores para todo tipo de im√°genes. Se puede encontrar aqu√≠ abajo.

* Algunos upscalers notables [se pueden encontrar aqu√≠](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/ESRGAN).
* LDSR es un upscaler avanzado pero lento, sus dos archivos [se encuentran aqu√≠](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/LDSR) y deben ser puestos en `stable-diffusion-webui/models/LDSR`.
* La [Upscale Wiki](https://upscale.wiki/wiki/Model_Database) contiene docenas de opciones hist√≥ricas.

En el futuro puede que muestre una comparaci√≥n de diferentes upscalers.

&nbsp;

# Scripts <a name="imgscripts"></a>[‚ñ≤](#index)

Los Scripts se encuentran al final de tus opciones de generaci√≥n de im√°genes, tanto en txt2img como img2img.

* **X/Y/Z Plot** <a name="plot"></a>[‚ñ≤](#index)

   Capaz de generar una serie de im√°genes, por lo general con la misma seed, pero cambiando algunos otros par√°metros que t√∫ elijas. Permite comparar casi cualquier cosa que desees, como diferentes modelos, partes de tu prompt, sampler, upscaler y mucho m√°s. Puedes tener 1, 2 √≥ 3 par√°metros variables, de all√≠ el X, Y, Z.

   Debes separar tus par√°metros con comas, y cualquier otra cosa puede ir entre medio. El par√°metro m√°s com√∫n es **S/R Prompt**, donde antes de la coma escribes una parte de tu prompt, y cada coma precede una frase que la reemplazar√°. Sabiendo esto, podemos comparar, por ejemplo, el peso/intensidad de un Lora, as√≠:
   
   `<lora:mi lora:0.4>, <lora:mi lora:0.6>, <lora:mi lora:0.8>, <lora:mi lora:1>`

   Aqu√≠ abajo he hecho una comparaci√≥n de diferentes **modelos** (en las columnas) y rostros de diferentes pa√≠ses con **S/R Prompt** (en las filas):
   
   Here I made a comparison between different **models** (columns) and faces of different ethnicities via **S/R Prompt** (rows):

   <details>
   <summary>Ejemplo de X/Y/Z Plot, click para expandir</summary>
   
   ![X Y Z plot de modelos y pa√≠ses](images/XYZplot.png)
   </details>

   **Consejo:** Parece posible hacer S/R incluyendo comas si utilizas comillas de la siguiente forma, sin espacio entre cada coma y comilla: `"frase 1, frase 2","frase 3, fase 4","frase 5, frase 6"`

* **Prompt Matrix** <a name="matrix"></a>[‚ñ≤](#index)

   Un concepto similar al S/R anterior, pero con mayor profundidad. Lo que hace es una cuadr√≠cula, la cual muestra cada posible combinaci√≥n de t√©rminos, donde los t√©rminos estar√°n separados con un `|` en tu prompt. Por ejemplo, `young man|tree, grass|city` - aqu√≠ "young man" siempre ser√° considerado, pero podremos ver qu√© pasa al a√±adir o quitar "tree, grass" y/o "city".
   
   Dentro del script puedes elegir hacerlo con tu prompt o tu negative prompt, y si quieres que los t√©rminos adicionales se introduzcan al inicio o al final.

   <a name="matrixneg"></a>Aqu√≠ hay una comparaci√≥n de los negative prompt que te mostr√© [anteriormente ‚ñ≤](#prompt). Podemos ver c√≥mo EasyNegative afecta la imagen, c√≥mo el resto del negative prompt afecta la imagen, y luego ambos juntos:

   <details>
   <summary>Ejemplo de Prompt Matrix, click para expandir</summary>
  
   ![Prompt matrix negativo de anime](images/promptmatrix1.png)
   ![Prompt matrix negativo de fotos](images/promptmatrix2.png)
   </details>

* **Ultimate Upscale** <a name="ultimate"></a>[‚ñ≤](#index)

   √âsta es una versi√≥n mejorada de un script b√°sico y debe ser a√±adida como una [extensi√≥n ‚ñ≤](#extensions) para ser usada desde **img2img**. Su prop√≥sito es agrandar una imagen as√≠ a√±adiendo m√°s detalles, sobrepasando el l√≠mite de tu gr√°fica dado que divide la imagen en secciones, aunque esto sea m√°s lento. Aqu√≠ las instrucciones:

   1. Genera tu imagen normalmente hasta un ancho y largo de 768, y aplica Hires fix si es que puedes.

   1. Desde txt2img o la extensi√≥n del navegador de im√°genes, env√≠a directamente la imagen con sus par√°metros a img2img.
   
   1. Ajusta el  **Denoising** entre 0.1 y 0.4. Valores m√°s grandes probablemente introduzcan mutaciones en tu imagen.

   1. Baja a **Scripts** y elije **Ultimate SD Upscale**. Config√∫ralo de la siguiente forma, con el tama√±o y upscaler que desees, y con el **Type "Chess"**:
   
      ![Ultimate upscale](images/ultimate.png)
      
      * Si tienes suficiente VRAM puedes aumentar el **Tile width** y el **Padding**, por ejemplo duplicando ambos. As√≠ ser√° m√°s r√°pido. El **Tile height** puede permanecer en 0.
     
      * No es necesario cambiar el **Seams fix** a menos que la imagen final muestre distorsiones visibles entre cada zona cuadrada.
     
   1. Genera tu imagen y espera que empiece. Podr√°s ver c√≥mo los cuadros se vuelven m√°s n√≠tidos si es que tienes activada la previsualizaci√≥n de im√°genes.
   
&nbsp;

# ControlNet <a name="controlnet"></a>[‚ñ≤](#index)

ControlNet es una tecnolog√≠a reciente extremadamente poderosa. Te permite analizar una imagen para guiar la creaci√≥n de tus propias im√°genes con Stable Diffusion. Veremos qu√© significa esto en un momento.

Si est√°s usando el collab de esta gu√≠a activa la casilla de `all_control_models`. Sino, deber√°s instalar la [extension ControlNet ‚ñ≤](#extensions), luego ir [aqu√≠](https://civitai.com/models/9251/controlnet-pre-trained-models) y descargar modelos de controlnet que deber√°s poner en la carpeta `stable-diffusion-webui/extensions/sd-webui-controlnet/models`. Recomiendo los modelos Canny, Depth, Openpose y Scribble, los cuales veremos en un momento.

Voy a demostrar c√≥mo ControlNet puede ser usado. Para ello tomar√© una imagen popular en internet como nuestra "imagen de muestra". No es necesario que me sigas paso a paso, pero puedes descargar las im√°genes y ponerlas en la pesta√±a **PNG Info** para ver los datos de generaci√≥n.

Primero, debes estar en txt2img y bajar para presionar el men√∫ ControlNet. Una vez abierto presiona *Enable*, y elige un *preprocessor* y *model* con el mismo nombre. Para empezar elegir√© Canny para ambos. Finalmente a√±adir√© mi imagen de muestra. Aseg√∫rate de no clickear sobre la imagen de muestra o comenzar√°s a dibujar. Podemos ignorar el resto de las opciones.

![Control Net](images/controlnet.png)

* **Canny**

   El m√©todo Canny extrae los detalles de la imagen de muestra. Es √∫til para imitar todo tipo de im√°genes. Observa:

   <details>
   <summary>Ejemplo de Canny, click para expandir</summary>
   
   ![Canny preprocessed image](images/canny1.png)
   ![Canny output image](images/canny2.png)
   </details>

* **Depth**

   El m√©todo Depth extrae los elementos 3D de la imagen de muestra. Es de enorme utilidad cuando deseas imitar ambientes complejos y la composici√≥n general de una imagen. Observa:

   <details>
   <summary>Ejemplo de Depth, click para expandir</summary>
   
   ![Depth preprocessed image](images/depth1.png)
   ![Depth output image](images/depth2.png)
   </details>

* **Openpose**

   El m√©todo Openpose extrae las poses humanas de la imagen de muestra. Es de extrema utilidad para obtener la toma deseada y composici√≥n de uno de tus personajes. Observa:

   <details>
   <summary>Ejemplo de Openpose, click para expandir</summary>
   
   ![Open Pose preprocessed image](images/openpose1.png)
   ![Open Pose output image](images/openpose2.png)
   </details>

* **Scribble**

   Scribble te permite hacer un bosquejo y convertirlo en una pieza terminada con ayuda de tu prompt. Este es el √∫nico ejemplo de aqu√≠ que no comparte la misma imagen de muestra.

   <details>
   <summary>Ejemplo de Scribble, click para expandir</summary>
   
   ![Scribble sample image](images/scribble1.jpg)
   ![Scribble output image](images/scribble2.png)
   </details>

Podr√°s notar que hay 2 resultados para cada m√©todo. El primero es en paso intermedio llamado la "imagen pre-procesada", la cual se usa para producir la imagen final. Puedes entregar una imagen pre-procesada t√∫ mismo, en tal caso deber√°s elegir un preprocessor de *None*. Esto puede ser tremendamente poderoso tomando en cuenta herramientas externas tales como Blender y Photoshop.

En la pesta√±a Settings habr√° una secci√≥n ControlNet donde podr√°s activar *m√∫ltiples controlnets al mismo tiempo*. Un uso particularmente √∫til es cuando uno de ellos es Openpose, para obtener tanto la pose deseada como el ambiente deseado, o con la posici√≥n exacta de manos u otros detalles. Observa:

<details>
<summary>Ejemplo de Openpose+Canny, click para expandir</summary>
  
![Open Pose + Canny](images/openpose_canny.png)
</details>

Tambi√©n puedes usar ControlNet en img2img, en tal caso la imagen de entrada y la imagen de muestra ambas tendr√°n ciertos efectos en el resultado. No tengo mucha experiencia con este m√©todo.

Adem√°s, existen la version **diff** de los modelos de controlnet, los cuales producen resultados ligeramente distintos. Puedes [probarlos](https://civitai.com/models/9868/controlnet-pre-trained-difference-models) si deseas, pero yo no lo he hecho.

&nbsp;

# Entrenamiento de Loras para novatos <a name="train"></a>[‚ñ≤](#index)

Entrenar un [Lora ‚ñ≤](#lora) t√∫ mismo es una especie de logro. No es la gran cosa, pero hay muchas variables involucradas, y mucho trabajo dependiendo de las t√©cnicas que utilices. Es una mezcla entre un arte y una ciencia.

Puedes entranar Loras en tu propio computador si tienes al menos 8 GB de VRAM. Sin embargo utilizar√© un documento de Google Collab por motivos educacionales.

He aqu√≠ unos recursos cl√°sicos si deseas leer sobre el tema en profundidad. Puede que Rentry est√© bloqueado por tu proveedor de internet, en tal caso puedes usar un VPN o intentar poner la p√°gina a trav√©s de [Google Translate](https://translate.google.cl/?op=websites).
* [Entrenamiento de Loras, en Rentry](https://rentry.org/lora_train)
* [Ciencia de Loras, en Rentry](https://rentry.org/lora-training-science)
* [Entrenador original de Kohya (m√©todo Dreambooth)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)
* [Lista de par√°metros del entrenador](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts#list-of-arguments)

Con dichos recursos mucho m√°s inteligentes puestos de lado, intentar√© producir una gu√≠a simple para que puedas hacer tu propio Lora, de un personaje, concepto o estilo.

![Trainer collab](images/trainercollab.png)

1. Utilizaremos [ESTE COLLAB](https://colab.research.google.com/drive/1WVTkW0IOeiBrs6s79XuJ9r1u42fKw81L?usp=sharing). Puedes copiarlo a tu Google Drive si deseas.

1. Presiona el bot√≥n de reproducci√≥n de *üá¶ Montar tu google drive* y dale acceso cuando lo pida. Haz lo mismo con *üáß Instalaci√≥n*. Mientras se instala, sigue al siguiente paso.

1. Baja a *üá® Configuraci√≥n* pero a√∫n no lo actives. Aqu√≠ en **Inicio** puedes darle cualquier nombre a tu proyecto. Tambi√©n puedes cambiar el modelo base que utilizaremos, pero para esta gu√≠a utilizaremos AnythingV3_fp16 ya que es la base de todos los modelos anime y produce los mejores resultados para ello. Si deseas entrenar con fotograf√≠as puedes copiar el enlace al modelo base de [Stable Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) o al modelo realista que desees utilizar (tal como [Deliberate](https://civitai.com/api/download/models/15236)). Recuerda tambi√©n cambiar el `model_type` a safetensors en tal caso.

1. **Archivos de entrenamiento** <a name="datasets"></a>[‚ñ≤](#index)
  
   Esta es la mayor parte del entrenamiento de Loras. Necesitar√°s recopilar un "dataset" o archivos de entrenamiento, los cuales consisten en im√°genes y sus correspondientes descripciones (con tags en el caso de anime).

   1. Encuentra im√°genes online que representes el personaje/concepto/estilo que deseas entrenar, posiblemente en sitios tales como [safebooru](https://safebooru.org/), [gelbooru](https://gelbooru.com/) o [danbooru](https://danbooru.donmai.us/). Necesitas al menos 10 im√°genes, idealmente 20 o m√°s, pero puedes usar cientos si deseas.
  
   1. Puedes crear los tags t√∫ mismo, lo cual es lento y poco preciso. Opcionalmente puedes agregar la [extensi√≥n Tagger](https://github.com/toriato/stable-diffusion-webui-wd14-tagger) a tu programa, la cual analiza todas tus im√°genes de entranemiento y genera tags para ellas.
  
   1. Opcionalmente puedes agregar otra extensi√≥n llamada [Tag Editor](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor) la cual te permite editar los tags de todos tus archivos al mismo tiempo.
  
   1. Una vez que tus im√°genes y descripciones est√©n listas, ponlas en una carpeta con la siguiente estructura: Una carpeta con el nombre de tu proyecto, la cual contiene al menos 1 carpeta en el formato `repeticiones_nombre`, la cual contiene tus archivos de entrenamiento. As√≠:
  
      ![Estructura de carpetas](images/trainfolder.png)

   1. Aqu√≠ es donde decides tu n√∫mero de repeticiones, con el nombre de la carpeta interior. Asumiendo que tienes solo 20 im√°genes, recomiendo 10 o 20 repeticiones. En tal caso, tu carpeta interior se llamar√° `10_minuevolora` o algo similar.
  
   1. Sube la carpeta exterior y todos sus contenidos (la que tiene el nombre de tu proyecto) a tu Google Drive, en la carpeta `lora_training/datasets/`.
  
1. **Opciones de entrenamiento** <a name="trainparams"></a>[‚ñ≤](#index)

   * Bajo **Archivos**, no necesitas cambiar nada esta vez.
   * Bajo **Pasos**, puedes cambiar los epochs y batch size seg√∫n lo descrito. M√°s epochs te dan m√°s control sobre el progreso de tu Lora, pero debes reducir las repeticiones.
   * Bajo **Entrenamiento**, el `unet_lr` or "learning rate" (velocidad de aprendizaje) es el par√°metro m√°s importanto. 1e-3 es el valor por defecto y funciona cuando tienes pocas im√°genes, pero puede ir hasta 1e-5. El dim es el tama√±o de tu Lora, y m√°s grande no necesariamente es mejor.

1. Ahora puedes activar *üá® Configuraci√≥n*, esperar que el modelo se descarge, y finalmente comenzar el entrenamiento con *üá© Cocinar el Lora*. Deber√≠a tomar 20 a 60 minutos. Si encuentras errores intenta contactarme o buscar ayuda.

1. **Probar tus resultados** <a name="traintest"></a>[‚ñ≤](#index)

   Ha pasado un rato y tu Lora termin√≥ de entrenar/cocinar. Ve y desc√°rgalo de la carpeta `lora_training/output` en tu google drive. Pero ver√°s que hay m√°s de uno; por defecto, se guarda una copia de tu Lora cada 2 epochs, permiti√©ndote as√≠ comparar su progreso. Si entrenas tu Lora por muchos epochs, podr√°s identificar el punto √≥ptimo entre que est√© "crudo" o "recocido".

   Cuando un Lora est√° "crudo", no alcanzar√° a imitar tus datos de entrenamiento. Cuando est√° "recocido", imita tus datos de entrenamiento *demasiado*, lo cual evita que pueda hacer cualquier otra cosa. Y si no a√±adiste suficientes datos o datos de baja calidad, ¬°puede que est√© crudo y recocido al mismo tiempo!

   Usando lo aprendido en [X/Y/Z Plot ‚ñ≤](#plot), podemos hacer una comparaci√≥n del progreso de nuestro Lora:

   ![Comparaci√≥n del resultado de Lora](images/loratrain.png)

   Mira eso, ¬°se vuelve cada vez m√°s detallado! La √∫ltima imagen no tiene ning√∫n Lora para comparar. Este parece ser un Lora de personaje exitoso, pero necesitar√≠amos probar una variedad de semillas, prompts y escenas para estar seguros.

   Es com√∫n que tu Lora "queme" o distorsione tus im√°genes al ser usado con pesos altos como 1, sobre todo si est√° recocido. Un peso entre 0.5 y 0.8 es aceptable para nosotros. Puede que necesites ajustar la velocidad de aprendizaje o el dim para esto, u otras variables no encontradas en este collab. Si est√°s leyendo esto y conoces los secretos de los Lora, h√°znoslo saber.

   Despu√©s de acostumbrarse a hacer Loras, e interactuar on la comunidad y sus variados recursos, estar√°s listo para usar otro m√©todo m√°s avanzado como el [collab original todo-en-uno de kohya](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Buena suerte.

* **Consejos adicionales** <a name="trainchars"></a>[‚ñ≤](#index)

   La parte m√°s importante para un personaje son los tags. Claro que necesitas im√°genes con variadas poses y lugares, pero si las descripciones est√°n mal no servir√° de nada.

   Cuando entrenas un personaje o concepto deber√≠as definir una **palabra de activaci√≥n**, y ajustar el valor de `keep_tokens` a 1. Una palabra de activaci√≥n es como podremos invocar a tu Lora para que funcione. Habiendo hecho eso, quieres quitar o "limpiar" las tags que son intr√≠nsicas a tu personaje o concepto, tales como el color de pelo y ojos. Por ejemplo, si una chica siempre tiene orejas de gato, quieres quitar las tags tales como `animal ears, animal ear fluff, cat ears`, y as√≠ √©stas ser√°n "absorbidas" por tu palabra de activaci√≥n.

   Tambi√©n puedes limpiar las tags de atuendo, dejando as√≠ s√≥lo los aspectos m√°s relevantes de la ropa y eliminando las redundancias, por ejemplo dejar "tie" pero quitar "red tie". Esto facilitar√° que estas ropas absorban los detalles relevantes. Incluso puedes definir una palabra de activaci√≥n para cada atuendo importante, por ejemplo personaje-normal, personaje-bikini, etc. Pero hay m√°s de una manera de lograr esto. En cualquier caso, con el uso correcto de tags, tu personaje deber√≠a ser capaz de cambiar de ropa f√°cilmente.

   Mientras tanto, los Loras de estilo no necesitan palabra de activaci√≥n, ya que deseamos que siempre est√©n activos. Absorber√°n el estilo art√≠stico de forma natural, y funcionar√° con variados pesos.

   Esta "absorci√≥n" de detalles no entregados por los tags es la forma en que los Loras funcionan en general, ya que logran aprender y representar los detalles imperceptibles o dif√≠ciles de explicar tales como el rostro, acccesorios, composici√≥n, etc.

&nbsp;

# ...vtubers? <a name="vtubers"></a>[‚ñ≤](#index)

Y as√≠ llegamos la final de la gu√≠a. Gracias por leer. Si tienes correcciones o contribuciones puedes abrir un Issue o un Pull Request en esta p√°gina y echar√© un vistazo pronto.

Tengo [otra p√°gina dedicada a Loras de vtubers, en especial Hololive](https://huggingface.co/hollowstrawberry/holotard). Si es que es de tu inter√©s.

Saludos.

&nbsp;
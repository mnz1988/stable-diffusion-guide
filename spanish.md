# 칈ndice <a name="index"></a>

* [Introducci칩n](#intro)
* [Google Colab](#colab)
* [Instalaci칩n Local (Windows + Nvidia)](#install)
* [Conceptos Esenciales](#start)
   1. [Modelos](#model)
   1. [VAEs](#vae)
   1. [Prompts](#prompt)
   1. [Opciones de generaci칩n](#gen)
* [Extensiones](#extensions)
* [Loras](#lora)
* [Im치genes Grandes](#upscale)
* [Scripts](#imgscripts)
   * [X/Y/Z Plot](#plot)
   * [Prompt Matrix](#matrix)
   * [Ultimate Upscaler](#ultimate)
* [ControlNet](#controlnet)
* [Entrenamiento de Loras para novatos](#train)
    * [Colab de entrenamiento](#traincolab)
    * [Archivos de entrenamiento](#datasets)
    * [Opciones de entrenamiento](#trainparams)
    * [Probar tus resultados](#traintest)
    * [Consejos adicionales](#trainchars)
* [...vtubers?](#vtubers)
 
&nbsp;

# Introducci칩n <a name="intro"></a>[郊쑇(#index)

Stable Diffusion es una poderosa herramienta de generaci칩n de im치genes a trav칠s de inteligencia artificial (IA), la cual puedes usar en tu propio hogar. 칄sta utiliza "modelos", los cuales son el cerebro de la IA y pueden crear casi cualquier cosa, siempre y cuando alguien los haya entrenado para ello. Los usos m치s populares son generaci칩n de arte anime, de fotorealismo, y de contenido para adultos.

Las im치genes que creas pueden ser usadas para cualquier prop칩sito, siempre y cuando sigan la licencia del modelo utilizado. Estas im치genes puede o no que sean "tuyas" en un sentido legal, dependiendo de las leyes de tu pa칤s, y com칰nmente es inconcluso. Ni yo ni nadie asociado con Stable Diffusion y sus modelos somos responsables por el contenido que generes, y se te prohibe usar estas herramientas para generar contenido ilegal o da침ino.

Esta gu칤a est치 actualizada hasta Marzo de 2023. Una semana es como un a침o para el desarrollo de IAs, as칤 que espero que siga siendo 칰til para cuando la leas.

&nbsp;

# Google Colab <a name="colab"></a>[郊쑇(#index)

La manera m치s facil de usar Stable Diffusion es a trav칠s de Google Colab. Con 칠l tomas prestado los computadores de Google para usar la IA, con tiempo limitado, com칰nmente varias horas al d칤a. Necesitar치s al menos una cuenta de Google y utilizaremos el Google Drive para guardar tus im치genes.

Si en su lugar deseas correr el programa en tu propio computador, [baja aqu칤 郊](#install).

Aqu칤 las instrucciones del colab. Estaremos usando el stable-diffusion-webui de Automatic1111.

1. Abre [esta p치gina con el colab de nocrypt en espa침ol](https://colab.research.google.com/drive/11-LDA0nvFprS_-g7-ZmTzsDVNpGzRvfO?usp=sharing).

1. Cerca de arriba clickea **Copiar a mi Drive**. Espera que se abra la ventana nueva y cierra la vieja. Ahora tienes tu propio colab el cual puedes configurar a tu gusto, y deber치s abrir desde tu Google Drive. En caso de actualizaciones deber치s ver el original.

1. Si deseas puedes activar las siguientes casillas bajo **Configuraciones**: `guardar_imagenes_en_drive, guardar_configuracion_en_drive`.

1. Si ya conoces Stable Diffusion puedes pegar los enlaces a tus recursos deseados en la casilla de texto de `enlaces_adicionales`, bajo **Modelos y otros recursos**. Puede que vayamos a a침adir enlaces aqu칤 m치s adelante en la gu칤a. Los enlaces deben ser **descargas directas** a cada archivo (idealmente de los sitios civitai o huggingface), y deben separarse por comas.

1. Ahora puedes presionar el bot칩n de reproducci칩n a la izquierda, en cualquier lugar dentro de la gran secci칩n llamada **Empezar aqu칤 游**. Espera un par de minutos para que se instale y corra el programa. Ver치s aparecer mensajes de progreso m치s abajo. Eventualmente uno de estos mensajes ser치 **Running on public URL** lo cual indica que est치 listo, y puedes abrir ese enlace en una nueva pesta침a para utilizar Stable Diffusion. **춰Mant칠n la pesta침a del colab abierta!** (en tel칠fono intenta el truco en la parte de abajo del colab para evitar que se cierre la pesta침a)

1. Ahora est치s listo para hacer algunas im치genes. Pero eso no es todo, y adem치s, 쯤u칠 son todas estas opciones? [Baja aqu칤 郊](#start) para aprender las bases.

&nbsp;

# Instalaci칩n Local (Windows + Nvidia) <a name="install"></a>[郊쑇(#index)

Para correr Stable Diffusion en tu propio computador necesitar치s al menos 16 GB de RAM y 4 GB de VRAM (idealmente 8). Por ahora s칩lo voy a explicar el caso en que uses Windows 10/11 y poseas una tarjeta gr치fica NVIDIA de serie 10XX o mayor. Mis disculpas si tienes AMD o usas Linux o Mac, pero con ellos es m치s complejo. Si no cumples estas condiciones a칰n puedes usar el Google Colab [aqu칤 arriba 郊쑇(#colab).

Aqu칤 las instrucciones de instalaci칩n. Utilizaremos un launcher para correr el stable-diffusion-webui de Automatic1111.

1. Obt칠n el instalador m치s reciente desde [esta p치gina](https://github.com/EmpireMediaScience/A1111-Web-UI-Installer/releases). 

1. Corre el instalador (dile a Windows que no es un virus) y selecciona una ubicaci칩n sencilla y accesible en donde instalar. Espera a que termine.

1. Corre el programa, 칠ste es el launcher. Ver치s algunas opciones. Primero activa **medvram** y **xformers**. Si tu gr치fica tiene 12 GB de VRAM o m치s no es necesario medvram.

1. En la casilla de texto que dice *Additional Launch Options* pega lo siguiente: `--opt-channelslast --no-half-vae --theme dark` . Si a침ades m치s opciones sep치ralas con espacios.
   * Si tu gr치fica tiene 4 o 6 GB de VRAM a침ade `--opt-split-attention-v1` lo cual puede ayudar un poco.
   * Si deseas correr el programa en un dispositivo y usarlo a trav칠s de otro dispositivo en la misma red de WiFi (como en tu tel칠fono), puedes a침adir `--listen --enable-insecure-extension-access` . Tras iniciar el programa podr치s conectarte desde el navegador usando la IP local en el puerto 7860. Tambi칠n puedes a침adir una contrase침a con `--gradio-auth nombre:contr` .
   * Puedes encontrar todas las opciones [aqu칤](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Command-Line-Arguments-and-Settings).

1. Presiona **Launch** y espera a que cargue y/o termine de instalar. Cuando termine se abrir치 una ventana en tu navegador.

1. La p치gina est치 abierta, es tu propio sitio web privado. Aqu칤 en la pesta침a principal (txt2img) es donde har치s casi todas tus im치genes. Pero primero iremos a la pesta침a **Settings**, y veremos algunas secciones del lado izquierdo.
   * En la secci칩n de *Stable Diffusion* baja al final y aumenta el **Clip skip** de 1 a 2. Se dice que produce mejores im치genes.
   * En la secci칩n *User Interface*, baja hasta **Quicksettings list** y c치mbialo a `sd_model_checkpoint, sd_vae` .
   * Vuelve a subir y presiona el gran **Apply settings**, luego **Reload UI**.

1. Ahora est치s m치s que listo para generar im치genes, pero s칩lo tienes el modelo b치sico. No es muy bueno, sirve para pinturas entre otras cosas. Adem치s, 쯤u칠 son todas estas opciones? Ve [aqu칤 abajo 郊](#start) para aprender las bases.

&nbsp;

# Conceptos Esenciales <a name="start"></a>[郊쑇(#index)

Antes o despu칠s de hacer tus primeras im치genes, querr치s leer la informaci칩n de aqu칤 abajo para mejorar tu experiencia y resultados. Si seguiste las instrucciones de esta gu칤a, la parte de arriba de tu p치gina de *Stable Diffusion WebUI* deber칤a verse parecida a esto:

![Arriba](images/top.png)

Aqu칤 puedes seleccionar un checkpoint y un VAE. Ahora explicar칠 qu칠 son ambas cosas y c칩mo obtenerlas. El colab tiene m치s opciones aqu칤 arriba pero puedes ignorarlas.

1. **Modelos** <a name="model"></a>[郊쑇(#index)

   El **modelo**, tambi칠n llamado **checkpoint**, es el cerebro de tu IA, dise침ado para producir cierto tipo de im치genes. Hay muchas opciones, las cuales puedes encontrar aqu칤 en huggingface o en [civitai](https://civitai.com). Ya que a칰n no sabes elegir, estas son mis recomendaciones:
   * Para hacer anime, [7th Heaven Mix](https://civitai.com/models/4669/corneos-7th-heaven-mix) tiene un estilo placentero, est칠ticamente parecido a las pel칤culas de anime, mientras que [Abyss Orange Mix 3](https://civitai.com/models/9942/abyssorangemix3-aom3) *(__Nota:__ Baja all칤 y elige la opci칩n AOM3)* ofrece m치s realismo con luces suaves, y m치s lascivia. Personalmente mezcl칠 estas dos opciones creando as칤 [Heaven Orange Mix](https://civitai.com/models/14305/heavenorangemix).
   * Aunque AOM3 es extremadamente capaz de hacer contenido para adultos, el popular modelo de hentai [Grapefruit](https://civitai.com/models/2583/grapefruit-hentai-model) tambi칠n puede cumplir tus deseos.
   * Para arte en general elige [DreamShaper](https://civitai.com/models/4384/dreamshaper), no hay nada que se le acerque en t칠rminos de creatividad. Tambi칠n est치 [Pastel Mix](https://civitai.com/models/5414/pastel-mix-stylized-anime-model), el cual tiene una hermosa y 칰nica est칠tica con un poco de anime.
   * Para el fotorealismo recomiendo [Deliberate](https://civitai.com/models/4823/deliberate). Puede hacer casi cualquier cosa, pero fotos en especial. Muy detallado.
   * El modelo [URPM](https://civitai.com/models/2661/uber-realistic-porn-merge-urpm) es la mayor concentraci칩n de pornograf칤a que vas a encontrar.
   
   Si est치s usando el colab de est치 gu칤a, algunos de estos est치n disponibles como opciones. Para otros modelos, debes copiar el **enlace directo a la descarga** y pegarlo en la casilla llamada `enlaces_adicionales`. Separa m칰ltiples enlaces usando comas.

   En una instalaci칩n local, com칰nmente los modelos deben ir dentro de la carpeta `stable-diffusion-webui/models/Stable-diffusion`.

   Una nota importante es que los checkpoints deben estar en formato `.safetensors`, ya que algunos archivos `.ckpt` **pueden** contener virus. Ten cuidado. Adem치s, cuando elijas modelos a veces ver치s varias opciones tales comoo fp32, fp16 y pruned. Para generar im치genes todas funcionan igual, as칤 que elige el archivo m치s peque침o (pruned-fp16). Para mezclar o entrenar modelos se recomienda el archivo m치s grande.

   **Consejo:** Tras a침adir el archivo de un recurso a las carpetas del programa, podr치s encontrarlo tras presionar 游댂 junto al lugar donde lo selecciones.

1. **VAEs** <a name="vae"></a>[郊쑇(#index)

   La mayor칤a de checkpoints no viene con VAE inclu칤do. El VAE es un peque침o modelo aparte, el cual "convierte tu imagen a formato humano". Sin un VAE tus im치genes van a tener malos colores y detalles.

   Si usas el colab de esta gu칤a, ya deber칤an estar todos los VAEs instalados.
   
   Pr치cticamente s칩lo hay 3 VAEs en circulamiento:
   * [anything vae](https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt), tambi칠n conocido como orangemix vae. Todos los modelos de anime lo utilizan.
   * [vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/blob/main/vae-ft-mse-840000-ema-pruned.safetensors), el oficial de Stable Diffusion, utilizado com칰nmente por modelos realistas.
   * [kl-f8-anime2](https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt), tambi칠n conocido como el vae de Waifu Diffusion, es m치s viejo y tiene colores m치s brillantes. Lo utiliza Pastel Mix.

   Si usas el launcher puedes elegir el VAE antes de iniciar el programa, de otra forma los VAEs deben ir en la carpeta `stable-diffusion-webui/models/VAE`.
   
   Si no has seguido esta gu칤a hasta este punto, dentro de tu p치gina deber치s ir a la pesta침a **Settings**, luego la seccci칩n **Stable Difussion**, y all칤 escoger tu VAE y guardar los cambios.

   **Consejo:** Tras a침adir el archivo de un recurso a las carpetas del programa, podr치s encontrarlo tras presionar 游댂 junto al lugar donde lo selecciones.

1. **Prompts** <a name="prompt"></a>[郊쑇(#index)

   Har치s casi todas tus im치genes en la primera pesta침a de tu p치gina, **txt2img**. Aqu칤 ver치s dos grandes casillas que llamamos *prompt* y *prompt negativo*. Aqu칤 deber치s describir lo que deseas que aparezca y no aparezca en tu imagen, **y debe ser en ingl칠s**.  
   Stable Diffusion no es como Midjourney u otros servicios populares, no puedes solo decir lo que quieres, sino que debes ser *muy* espec칤fico.  
   Debido a esto la mayor칤a de personas se aferran a alg칰n prompt que le funcione bien a ellos, muchas veces recomendado por otra persona. Aqu칤 recomiendo mis propios prompts y prompts negativos:
   
   * Anime
      * `2d, masterpiece, best quality, anime, highly detailed face, highly detailed background, perfect lighting`
      * `EasyNegative, worst quality, low quality, 3d, realistic, photorealistic, (loli, child, teen, baby face), zombie, animal, multiple views, text, watermark, signature, artist name, artist logo, censored`
     
   * Fotorealismo
      * `best quality, 4k, 8k, ultra highres, raw photo in hdr, sharp focus, intricate texture, skin imperfections`
      * `EasyNegative, worst quality, low quality, normal quality, child, painting, drawing, sketch, cartoon, anime, render, 3d, blurry, deformed, disfigured, morbid, mutated, bad anatomy, bad art`

   * **EasyNegative:** <a name="promptneg"></a>El prompt negativo que recomiendo utiliza EasyNegative, un *embedding* o "palabra m치gica" que codifica muchas cosas malas para as칤 mejorar tus im치genes. De otra forma tu prompt negativo ser칤a enorme.
      * Si usas el colab de esta gu칤a, ya tienes instalado EasyNegative. Sino, es un archivo diminuto que puedes [descargar aqu칤](https://huggingface.co/datasets/gsdf/EasyNegative/resolve/main/EasyNegative.safetensors) y debes colocar en la carpeta `stable-diffusion-webui/embeddings`. Finalmente debes reiniciar el programa para as칤 poder usar esta palabra m치gica.

   Puedes ver una comparaci칩n de prompts negativos incluyendo EasyNegative m치s abajo en [Prompt Matrix 郊](#matrixneg).

   ![Prompts](images/prompt.png)

   Despu칠s de un "prompt base" como los que te he mostrado, puedes comenzar a escribir lo que desees. Por ejemplo, `young woman in a bikini in the beach, full body shot`. Tambi칠n puedes a침adir m치s t칠rminos negativos, como `old, ugly, futanari, furry`, etc.  
   Puedes guardar tus prompts usando los botones debajo de Generate. Presiona el peque침o 游 *Save style* y asigna un nombre al prompt actual. Tras ello podr치s abrir tus *Styles* para elegirlo, y luego presionar 游늶 *Apply selected styles to the current prompt* para a침adirlo.

   <a name="promptweight"></a>Una t칠cnica importante en los prompts es la intensidad. Cuando encierras algo en `(par칠ntesis)`, tendr치 m치s intensidad o **peso** en tu imagen, dici칠ndole a la IA que esa parte es m치s importante. El peso normal para todas las palabras es 1, y cada par칠ntesis multiplica por 1.1 (puedes usar varios). Tambi칠n puedes especificar el peso t칰 mismo, por ejemplo: `(full body:1.4)`. Tambi칠n puedes hacerlo menor a 1 para bajar la intensidad; los `[corchetes]` multiplican por 0.9, pero si quieres ser preciso tambi칠n necesitas par칠ntssis, como `(as칤:0.5)`.

   Podr치s notar que la IA es famosamente mala para hacer manos y pies. Con estos buenos prompt mejorar치n un poco, pero quiz치 debas usar photoshop, inpainting, o t칠cnicas avanzadas como [ControlNet 郊](#controlnet) para perfeccionar tu imagen.

1. **Opciones de generaci칩n** <a name="gen"></a>[郊쑇(#index)

   Ya te ense침칠 a elegir un modelo, VAE y escribir tus prompt, ahora podr치s saber sobre todo el resto de las opciones disponibles antes de generar una imagen.
   
   ![Parameters](images/parameters.png)

   * **Sampling method:** Es el algoritmo que genera tu imagen, cada uno con resultados distintos. El por defecto de `Euler a` casi siempre es el mejor, y tambi칠n obtendr치s muy buenos resultados con `DPM++ 2M Karras` y `DPM++ SDE Karras`. Una comparaci칩n m치s abajo.
   * **Sampling steps:** La cantidad de pasos, son "calculados" con anticipacic칩n y por lo tanto m치s pasos no siempre es mejor. Yo siempre uso 30 pasos, pero de 20 a 50 encontrar치s resultados consistentemente buenos. Una comparaci칩n m치s abajo.
   * **Width and Height:** La resoluci칩n de 512x512 es lo normal. Si superas el ancho o alto de 768 tu imagen puede ser distorsionada y deformada. Para producir im치genes m치s grandes est치 la opci칩n `Hires fix`.
   * **Batch Count and Batch Size:** El *size* es cu치ntas im치genes tu tarjeta gr치fica producir치 al mismo tiempo, lo cual se limita for su VRAM. El *count* son las repeticiones del valor anterior. Los batches tienen seeds consecutivas, m치s abajo ver치s las seeds.
   * **CFG Scale:** "Los valores menores producen resultados m치s creativos". Casi siempre debes dejarlo en 7, pero de 4 a 10 es un rango aceptable.
   * **Seed:** Un n칰mero que dicta la generaci칩n de tu imagen. La misma seed con el mismo prompt y opciones siempre producir치 la misma imagen, salvo detalles menores y algunas excepciones.
  
   **Hires fix:** Esta opci칩n te permite crear im치genes m치s grandes sin problemas. Por lo general se ocupa para duplicar el ancho y alto. Cuando la actives, aparecer치n m치s opciones:
   * **Upscaler:** El algoritmo para agrandar la imagen. Se dice que `Latent` crea resultados creativos. Puede que tambi칠n te guste `R-ESRGAN 4x+` y su variante para anime. [M치s informaci칩n y comparaciones m치s abajo 郊](#upscale).
   * **Hires steps:** Recomiendo al menos la mitad de tus pasos normales. M치s pasos no siempre es mejor, y son bastante lentos, as칤 que s칠 conservador.
   * **Denoising strength:** El valor m치s importante. Cera de 0.0 tu imagen no tendr치 ning칰n detalle nuevo. Cerca de 1.0, tu imagen cambiar치 completamente. Recomiendo un valor entre 0.2 y 0.6 dependiendo del caso, lo cual a침ade suficiente detalle sin *destruir* los detalles existentes que te gusten.
    
   Others:
   * **Restore faces:** Puede mejorar los rostros reales. Nunca lo he necesitado con los prompt de esta gu칤a y con hires fix.
   * **Tiling:** Sirve para hacer patrones repetitivos como baldosas, no es muy 칰til.
   * **Script:** Te permite acceder a funciones y extensiones muy 칰tiles, [las cuales explico m치s abajo 郊](#plot). Por ejemplo, X/Y/Z Plot permite comparar una cuadr칤cula de im치genes con diferentes opciones. Muy poderoso.

   Aqu칤 comparo algunos samplers populares y varios cantidades de pasos:

   <details>
   <summary>(Click) Comparaci칩n de samplers - Fotograf칤a</summary>

   ![samplers con fotos](images/samplers1.png)
   </details>

   <details>
   <summary>(Click) Comparaci칩n de samplers - Anime</summary>

   ![samplers con anime](images/samplers2.png)
   </details>

   Corta explicaci칩n de la comparaci칩n de arriba: `Euler` es un sampler b치sico. `DDIM` es m치s rapido mientras que `DPM++ 2M Karras` es mejorado. Mientras tanto `Euler a` o "ancestral" produce resultados m치s creativos, y `DPM++ 2S a Karras` algo similar ya que tambi칠n es ancestral. Finalmente `DPM++ SDE Karras` es el m치s lento y bastante 칰nico. Hay muchos otros samplers pero la mayor칤a est치n relacionados.

&nbsp;
  
# Extensiones <a name="extensions"></a>[郊쑇(#index)

*Stable Diffusion WebUI* es el programa que estamos ocupando y 칠ste permite a침adir extensiones muy 칰tiles. Para ello dir칤gete a la pesta침a **Extensions**, luego a **Install from URL**, y pega all칤 alguno de estos enlaces de github. Luego presiona *Install* y espera que se instale. Cuando termines ve a **Installed** y presiona *Apply and restart UI*.
 
![Extensiones](images/extensions.png)

Aqu칤 hay algunas extensiones 칰tiles. Si usas el colab de esta gu칤a la mayor칤a ya est치n instaladas, sino, recomiendo enormemente instalar manualmente las primeras 2.
* [Image Browser (bugfix)](https://github.com/aka7774/sd_images_browser) - Navegador de Im치genes, permite ver todas las im치genes wue has creado y r치pidamente enviarlas con sus par치metros a txt2img, img2img, etc.
* [TagComplete](https://github.com/DominikDoom/a1111-sd-webui-tagcomplete) - Completamente esencial para hacer anime, te muestra las tags de booru existentes mientras escribes tu prompt. Los modelos de anime funcionan a trav칠s de estos tags, haciendo de 칠sta una de las mejores extensiones. Ojo que no todas las tags funcionan siempre, sobre todo si son poco comunes.
* [ControlNet](https://github.com/Mikubill/sd-webui-controlnet) - Enorme extensi칩n con [su propia gu칤a 郊](#controlnet). Te permite analizar cualquier imagen existente y usarla como muestra para guiar tus propias im치genes. En t칠rminos pr치cticos, te permite replicar cualquier pose o ambiente que desees.
* [Ultimate Upscale](https://github.com/Coyote-A/ultimate-upscale-for-automatic1111) - Un script usable desde img2img que permite hacer im치genes enormes aunque tengas poca vram, dividi칠ndolas en secciones aunque sea m치s lento. [Ver su gu칤a aqu칤 郊](#ultimate).
* [Two-shot](https://github.com/opparco/stable-diffusion-webui-two-shot) - Normalmente no es posible crear escenas de dos personajes, ya que el prompt hace que se fusionen sus caracter칤sticas. Esta extensi칩n permite dividir la imagen en: todo, izquierda, derecha; permitiendo as칤 tener escenas naturales con 2 personajes o temas al mismo tiempo.
* [Dynamic Prompts](https://github.com/adieyal/sd-dynamic-prompts) - Un script para tener prompts semi-aleatorios. Un poco complejo.
* [Model Converter](https://github.com/Akegarasu/sd-webui-model-converter) - Permite convertir modelos de 7 GB o 4 GB a 2 GB, seleccionando  `safetensors`, `fp16`, y `no-ema`. Estos modelos "pruneados" funcionan pr치cticamente igual para generar im치genes. La mayor칤a de modelos hoy en d칤a vienen en este formato de todas formas.

&nbsp;

# Loras <a name="lora"></a>[郊쑇(#index)

Los Loras son una tecnolog칤a moderna y un tipo de **Extra Network** que permite a침adir una especie de modelo peque침o a cualquiera de tus modelos principales. Son similares a los embeddings, uno de los cuales te mostr칠 [antes 郊쑇(#promptneg), pero los Loras son m치s grandes y com칰nmente m치s capaces. No entrar칠 en detalles t칠cnicos.

Un Lora puede representar un personaje, estilo, pose, ropa, o incluso un rostro humano (aunque no estoy de acuerdo con ello). Los checkpoints son bastante capaces para contenido general, pero para detalles como estos es donde comienzan a fallar y necesitar치s un Lora. Podr치s descargar Loras desde [civitai](https://civitai.com) u [otros lugares (NSFW)](https://gitgud.io/gayshit/makesomefuckingporn#lora-list) y su tama침o es de 144 MB por defecto, pero pueden ser tan peque침os como 1 MB. Los Loras m치s grandes no son necesariamente mejores. Los Loras vienen en formato `.safetensors` de igual forma que los checkpoints.

Coloca tus archivos de Lora en la carpeta `stable-diffusion-webui/models/Lora`, o si est치s usando el colab de esta gu칤a pega el enlace directo a la descarga en la casilla `enlaces_adicionales`. Luego encuentra el bot칩n 游꿦 *Show extra networks* bajo el gran bot칩n naranjo, el cual abrir치 una nueva secci칩n de extra networks. Presiona la pesta침a Lora y presiona **Refresh** para escanear nuevos Loras. Cuando hagas click en uno de tus Loras se a침adir치 a tu prompt, y se ver치 as칤: `<lora:archivo:1>` . Siempre se ver치n as칤, donde "archivo" es el nombre exacto del archivo en tu sistema (antes de `.safetensors`). Finalmente, el n칰mero es el peso, lo cual expliqu칠 [previamente 郊쑇(#promptweight). La mayor칤a de Loras funcionan con un peso entre 0.5 y 1, y los valores muy grandes pueden "cocinar" tu imagen, especialmente si usas m치s de uno al mismo tiempo.

![Extra Networks](images/extranetworks.png)

Adem치s, muchos Loras tendr치n una "palabra de activaci칩n" para que tomen efecto, por ejemplo el nombre del personaje en caso de ser un Lora de personaje.

Un ejemplo de Lora es [Thicker Lines Anime Style](https://civitai.com/models/13910/thicker-lines-anime-style-lora-mix), un gran estilo de 치nime cl치sico si deseas probarlo. No tiene palabra de activaci칩n.

&nbsp;

# Im치genes Grandes <a name="upscale"></a>[郊쑇(#index)

Como [mencionamos anteriormente 郊쑇(#gen), normalmente no debes generar im치genes sobre 768 de ancho y alto. Debes usar Hires fix, con un "upscaler" (algoritmo) y denoising (intensidad) apropiados. Hires fix est치 limitado por tu VRAM, por lo que te puede interesar [Ultimate Upscaler 郊](#ultimate).

Es posible descargar upscalers adicionales y ponerlos en tu carpeta `stable-diffusion-webui/models/ESRGAN`. As칤 funcionar치n con Hires fix, Ultimate Upscaler, y Extras.

El colab de esta gu칤a viene con varios de estos, incluyendo **Remacri**, uno de los mejores para todo tipo de im치genes. Se puede encontrar aqu칤 abajo.

* Algunos upscalers notables [se pueden encontrar aqu칤](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/ESRGAN).
* LDSR es un upscaler avanzado pero lento, sus dos archivos [se encuentran aqu칤](https://huggingface.co/hollowstrawberry/upscalers-backup/tree/main/LDSR) y deben ser puestos en `stable-diffusion-webui/models/LDSR`.
* La [Upscale Wiki](https://upscale.wiki/wiki/Model_Database) contiene docenas de opciones hist칩ricas.

Aqu칤 hay algunas comparaciones. Todas fueron hechas con denoising de 0.4. Ojo que algunas diferencias pueden ser completamente debido al azar. Desafortunadamente no puedo mostrar LDSR por limitaciones de VRAM.

<details>
   <summary>(Click) Comparaci칩n 1: Anime, estilizado, fantas칤a</summary>

   ![Original](images/upscalers1pre.png)
   ![Comparaci칩n](images/upscalers1.png)
</details>

<details>
   <summary>(Click) Comparaci칩n 2: Anime, detallado, iluminaci칩n suave</summary>

   ![Original](images/upscalers2pre.png)
   ![Comparaci칩n](images/upscalers2.png)
</details>

<details>
   <summary>(Click) Comparaci칩n 3: Fotograf칤a, humano, naturaleza</summary>

   ![Original](images/upscalers3pre.png)
   ![Comparaci칩n](images/upscalers3.png)
</details>

&nbsp;

# Scripts <a name="imgscripts"></a>[郊쑇(#index)

Los Scripts se encuentran al final de tus opciones de generaci칩n de im치genes, tanto en txt2img como img2img.

* **X/Y/Z Plot** <a name="plot"></a>[郊쑇(#index)

   Capaz de generar una serie de im치genes, por lo general con la misma seed, pero cambiando algunos otros par치metros que t칰 elijas. Permite comparar casi cualquier cosa que desees, como diferentes modelos, partes de tu prompt, sampler, upscaler y mucho m치s. Puedes tener 1, 2 칩 3 par치metros variables, de all칤 el X, Y, Z.

   Debes separar tus par치metros con comas, y cualquier otra cosa puede ir entre medio. El par치metro m치s com칰n es **S/R Prompt**, donde antes de la coma escribes una parte de tu prompt, y cada coma precede una frase que la reemplazar치. Sabiendo esto, podemos comparar, por ejemplo, el peso/intensidad de un Lora, as칤:
   
   `<lora:mi lora:0.4>, <lora:mi lora:0.6>, <lora:mi lora:0.8>, <lora:mi lora:1>`

   Aqu칤 abajo he hecho una comparaci칩n de diferentes **modelos** (en las columnas) y rostros de diferentes pa칤ses con **S/R Prompt** (en las filas):
   
   <details>
   <summary>(Click) Ejemplo de X/Y/Z Plot</summary>
   
   ![X Y Z plot de modelos y pa칤ses](images/XYZplot.png)
   </details>

   **Consejo:** Parece posible hacer S/R incluyendo comas si utilizas comillas de la siguiente forma, sin espacio entre cada coma y comilla: `"frase 1, frase 2","frase 3, fase 4","frase 5, frase 6"`

* **Prompt Matrix** <a name="matrix"></a>[郊쑇(#index)

   Un concepto similar al S/R anterior, pero con mayor profundidad. Lo que hace es una cuadr칤cula, la cual muestra cada posible combinaci칩n de t칠rminos, donde los t칠rminos estar치n separados con un `|` en tu prompt. Por ejemplo, `young man|tree, grass|city` - aqu칤 "young man" siempre ser치 considerado, pero podremos ver qu칠 pasa al a침adir o quitar "tree, grass" y/o "city".
   
   Dentro del script puedes elegir hacerlo con tu prompt o tu negative prompt, y si quieres que los t칠rminos adicionales se introduzcan al inicio o al final.

   <a name="matrixneg"></a>Aqu칤 hay una comparaci칩n de los negative prompt que te mostr칠 [anteriormente 郊쑇(#prompt). Podemos ver c칩mo EasyNegative afecta la imagen, c칩mo el resto del negative prompt afecta la imagen, y luego ambos juntos:

   <details>
   <summary>(Click) Ejemplo de Prompt Matrix</summary>
  
   ![Prompt matrix negativo de anime](images/promptmatrix1.png)
   ![Prompt matrix negativo de fotos](images/promptmatrix2.png)
   </details>

   **Tip:** Aumentando el Batch Size puedes crear todo el prompt matrix al mismo tiempo.

* **Ultimate Upscale** <a name="ultimate"></a>[郊쑇(#index)

   칄sta es una versi칩n mejorada de un script b치sico y debe ser a침adida como una [extensi칩n 郊쑇(#extensions) para ser usada desde **img2img**. Su prop칩sito es agrandar una imagen as칤 a침adiendo m치s detalles, sobrepasando el l칤mite de tu gr치fica dado que divide la imagen en secciones, aunque esto sea m치s lento. Aqu칤 las instrucciones:

   1. Genera tu imagen normalmente hasta un ancho y largo de 768, y aplica Hires fix si es que puedes.

   1. Desde txt2img o la extensi칩n del navegador de im치genes, env칤a directamente la imagen con sus par치metros a img2img.
   
   1. Ajusta el  **Denoising** entre 0.1 y 0.4. Valores m치s grandes probablemente introduzcan mutaciones en tu imagen.

   1. Baja a **Scripts** y elije **Ultimate SD Upscale**. Config칰ralo de la siguiente forma, con el tama침o y upscaler que desees, y con el **Type "Chess"**:
   
      ![Ultimate upscale](images/ultimate.png)
      
      * Si tienes suficiente VRAM puedes aumentar el **Tile width** y el **Padding**, por ejemplo duplicando ambos. As칤 ser치 m치s r치pido. El **Tile height** puede permanecer en 0.
     
      * No es necesario cambiar el **Seams fix** a menos que la imagen final muestre distorsiones visibles entre cada zona cuadrada.
     
   1. Genera tu imagen y espera que empiece. Podr치s ver c칩mo los cuadros se vuelven m치s n칤tidos si es que tienes activada la previsualizaci칩n de im치genes.
   
&nbsp;

# ControlNet <a name="controlnet"></a>[郊쑇(#index)

ControlNet es una tecnolog칤a reciente extremadamente poderosa. Te permite analizar una imagen para guiar la creaci칩n de tus propias im치genes con Stable Diffusion. Veremos qu칠 significa esto en un momento.

Si est치s usando el colab de esta gu칤a activa la casilla de `todos_modelos_controlnet` antes de iniciarlo. Sino, deber치s instalar la [extension ControlNet 郊쑇(#extensions), luego ir [aqu칤](https://civitai.com/models/9251/controlnet-pre-trained-models) y descargar modelos de controlnet que deber치s poner en la carpeta `stable-diffusion-webui/extensions/sd-webui-controlnet/models`. Recomiendo los modelos Canny, Depth, Openpose y Scribble, los cuales veremos en un momento.

Voy a demostrar c칩mo ControlNet puede ser usado. Para ello tomar칠 una imagen popular en internet como nuestra "imagen de muestra". No es necesario que me sigas paso a paso, pero puedes descargar las im치genes y ponerlas en la pesta침a **PNG Info** para ver los datos de generaci칩n.

Primero, debes estar en txt2img y bajar para presionar el men칰 ControlNet. Una vez abierto presiona *Enable*, y elige un *preprocessor* y *model* con el mismo nombre. Para empezar elegir칠 Canny para ambos. Finalmente a침adir칠 mi imagen de muestra. Aseg칰rate de no clickear sobre la imagen de muestra o comenzar치s a dibujar. Podemos ignorar el resto de las opciones.

![Control Net](images/controlnet.png)

* **Canny**

   El m칠todo Canny extrae los detalles de la imagen de muestra. Es 칰til para imitar todo tipo de im치genes. Observa:

   <details>
   <summary>(Click) Ejemplo de Canny</summary>
   
   ![Canny preprocessed image](images/canny1.png)
   ![Canny output image](images/canny2.png)
   </details>

* **Depth**

   El m칠todo Depth extrae los elementos 3D de la imagen de muestra. Es de enorme utilidad cuando deseas imitar ambientes complejos y la composici칩n general de una imagen. Observa:

   <details>
   <summary>(Click) Ejemplo de Depth</summary>
   
   ![Depth preprocessed image](images/depth1.png)
   ![Depth output image](images/depth2.png)
   </details>

* **Openpose**

   El m칠todo Openpose extrae las poses humanas de la imagen de muestra. Es de extrema utilidad para obtener la toma deseada y composici칩n de uno de tus personajes. Observa:

   <details>
   <summary>(Click) Ejemplo de Openpose</summary>
   
   ![Open Pose preprocessed image](images/openpose1.png)
   ![Open Pose output image](images/openpose2.png)
   </details>

* **Scribble**

   Scribble te permite hacer un bosquejo y convertirlo en una pieza terminada con ayuda de tu prompt. Este es el 칰nico ejemplo de aqu칤 que no comparte la misma imagen de muestra.

   <details>
   <summary>(Click) Ejemplo de Scribble</summary>
   
   ![Scribble sample image](images/scribble1.png)
   ![Scribble output image](images/scribble2.png)
   </details>

Podr치s notar que hay 2 resultados para cada m칠todo. El primero es en paso intermedio llamado la "imagen pre-procesada", la cual se usa para producir la imagen final. Puedes entregar una imagen pre-procesada t칰 mismo, en tal caso deber치s elegir un preprocessor de *None*. Esto puede ser tremendamente poderoso tomando en cuenta herramientas externas tales como Blender y Photoshop.

En la pesta침a Settings habr치 una secci칩n ControlNet donde podr치s activar *m칰ltiples controlnets al mismo tiempo*. Un uso particularmente 칰til es cuando uno de ellos es Openpose, para obtener tanto la pose deseada como el ambiente deseado, o con la posici칩n exacta de manos u otros detalles. Observa:

<details>
<summary>(Click) Ejemplo de Openpose+Canny</summary>
  
![Open Pose + Canny](images/openpose_canny.png)
</details>

Tambi칠n puedes usar ControlNet en img2img, en tal caso la imagen de entrada y la imagen de muestra ambas tendr치n ciertos efectos en el resultado. No tengo mucha experiencia con este m칠todo.

Adem치s, existen la version **diff** de los modelos de controlnet, los cuales producen resultados ligeramente distintos. Puedes [probarlos](https://civitai.com/models/9868/controlnet-pre-trained-difference-models) si deseas, pero yo no lo he hecho.

&nbsp;

# Entrenamiento de Loras para novatos <a name="train"></a>[郊쑇(#index)

Entrenar un [Lora 郊쑇(#lora) t칰 mismo es una especie de logro. No es la gran cosa, pero hay muchas variables involucradas, y mucho trabajo dependiendo de las t칠cnicas que utilices. Es una mezcla entre un arte y una ciencia.

Puedes entranar Loras en tu propio computador si tienes al menos 8 GB de VRAM. Sin embargo utilizar칠 un documento de Google Colab por motivos educacionales.

He aqu칤 unos recursos cl치sicos si deseas leer sobre el tema en profundidad. Puede que Rentry est칠 bloqueado por tu proveedor de internet, en tal caso puedes usar un VPN o intentar poner la p치gina a trav칠s de [Google Translate](https://translate.google.cl/?op=websites).
* [Entrenamiento de Loras, en Rentry](https://rentry.org/lora_train)
* [Ciencia de Loras, en Rentry](https://rentry.org/lora-training-science)
* [Entrenador original de Kohya (m칠todo Dreambooth)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb)
* [Lista de par치metros del entrenador](https://github.com/derrian-distro/LoRA_Easy_Training_Scripts#list-of-arguments)

Con dichos recursos mucho m치s inteligentes puestos de lado, intentar칠 producir una gu칤a simple para que puedas hacer tu propio Lora, de un personaje, concepto o estilo.

**Colab de entrenamiento** <a name="traincolab"></a>[郊쑇(#index)
![Trainer colab](images/trainercollab.png)

1. Utilizaremos [ESTE DOCUMENTO DE COLAB](https://colab.research.google.com/drive/1dkiziaR1rizn7f6jRM7jwH6KxpaoY0-w?usp=sharing). Puedes copiarlo a tu Google Drive si deseas.

1. Presiona el bot칩n de reproducci칩n de *游 Montar tu google drive* y dale acceso cuando lo pida. Haz lo mismo con *游 Instalaci칩n*. Mientras se instala en el servidor de Google, sigue al siguiente paso.

1. Baja a *游 Configuraci칩n* pero a칰n no lo actives. Aqu칤 en **Inicio** puedes darle cualquier nombre a tu proyecto. Tambi칠n puedes cambiar el modelo base que utilizaremos, pero para esta gu칤a utilizaremos `animefull-final` ya que es la base de casi todos los modelos anime y produce los resultados m치s consistentes. De otra forma recomendar칤a `AnythingV3`. Si deseas entrenar con fotograf칤as puedes copiar el enlace al modelo base de [Stable Diffusion 1.5](https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) o al modelo realista que desees utilizar (tal como [Deliberate](https://civitai.com/api/download/models/15236)).

1. **Archivos de entrenamiento** <a name="datasets"></a>[郊쑇(#index)
  
   Esta es la mayor parte del entrenamiento de Loras. Necesitar치s recopilar un "dataset" o archivos de entrenamiento, los cuales consisten en im치genes y sus correspondientes descripciones (con tags en el caso de anime).

   1. Encuentra im치genes online que representens el personaje/concepto/estilo que deseas entrenar, posiblemente en sitios tales como [gelbooru](https://gelbooru.com/). Aseg칰rate que sean im치genes de calidad decente en diferentes 치ngulos, escenas, ropa, etc. Necesitas al menos 10 im치genes, recomiendo 20 o m치s, y por lo general mientras m치s mejor.
      * Opcionalmente, puedes instalar [Grabber](https://github.com/Bionus/imgbrd-grabber/releases) para descargar cientos de im치genes autom치ticamente. Recomiendo buscar en gelbooru y pixiv, para un personaje ser칤a con estas tags: `1girl solo character_name score:>10 -rating:explicit` (lo expl칤cito suele ser raro y por lo tanto se puede excluir).
  
   1. Crea los archivos de texto junto a cada imagen, con el mismo nombre de archivo. Puedes escribir los tags t칰 mismo, aunque puede ser lento y poco preciso. Si son fotograf칤as, descr칤belas en detalle con oraciones simples.
      * Opcionalmente puedes agregar la [extensi칩n Tagger](https://github.com/toriato/stable-diffusion-webui-wd14-tagger) para que analice tus im치genes y cree las tags de anime por ti. Las instrucciones son as칤: A침ade y activa la extensi칩n, y reinicia el programa. Luego dir칤gete a la nueva pesta침a **Tagger**, luego a *atch from directory*, y selecciona la carpeta con tus im치genes. Pon el *output name* como `[name].txt` y el threshold a 0.2 o mayor (칠ste es la precisi칩n de las tags). Finalmente presiona **Interrogate** y se crear치n tus archivos de texto.
    
   1. Una vez que tus im치genes y descripciones est칠n listas, ponlas en una carpeta con la siguiente estructura: Una carpeta con el nombre de tu proyecto, la cual contiene al menos 1 carpeta en el formato `repeticiones_nombre`, la cual contiene tus archivos de entrenamiento. As칤:
  
      ![Estructura de carpetas](images/trainfolder_spanish.png)

   1. Aqu칤 es donde decides tu n칰mero de repeticiones, con el nombre de la carpeta interior. Recomiendo que tu cantidad de im치genes multiplicada por su n칰mero de repeticiones no supere un total de 400. As칤 que si tienes 20 im치genes, recomiendo 10 o 20 repeticiones, y m치s im치genes necesitar칤an menos. En tal caso, tu carpeta interior se llamar치 `10_repeticiones` (puedes cambiar la palabra "repeticiones" por un nombre a tu gusto).
  
   1. Sube la carpeta exterior y todos sus contenidos (la que tiene el nombre de tu proyecto) a tu Google Drive, en la carpeta `lora_training/datasets/`.
  
1. **Opciones de entrenamiento** <a name="trainparams"></a>[郊쑇(#index)

   * Bajo **Archivos**, no necesitas cambiar nada esta vez.
   * Bajo **Pasos**, puedes cambiar los epochs y batch size seg칰n lo descrito. M치s epochs te dan m치s control sobre el progreso de tu Lora, y dar치 m치s tiempo para que aprenda, pero no exageres. Si tienes muchas im치genes puedes aumentar el batch size.
   * Bajo **Entrenamiento**, el `unet_aprendizaje` es el par치metro m치s importante. 1e-3 es el valor por defecto y funciona cuando tienes pocas im치genes, pero puede ir hasta 1e-5. Tambi칠n est치 el dim, el cual es el tama침o de tu Lora, y m치s grande no necesariamente es mejor (recomiendo dim 16, alpha 8).

1. Ahora puedes activar *游 Configuraci칩n*, esperar que el modelo se descarge, y finalmente comenzar el entrenamiento con *游 Cocinar el Lora*. Deber칤a tomar 20 a 60 minutos si pusiste un n칰mero apropiado de repeticiones y epochs. Si encuentras errores intenta contactarme o buscar ayuda.

1. **Probar tus resultados** <a name="traintest"></a>[郊쑇(#index)

   Ha pasado un rato y tu Lora termin칩 de entrenar/cocinar. Ve y desc치rgalo de la carpeta `lora_training/output` en tu google drive. Pero ver치s que hay m치s de uno; por defecto, se guarda una copia de tu Lora cada 2 epochs, permiti칠ndote as칤 comparar su progreso. Si entrenas tu Lora por muchos epochs, podr치s identificar el punto 칩ptimo entre que est칠 "crudo" o "recocido".

   Cuando un Lora est치 "crudo", no alcanzar치 a imitar tus datos de entrenamiento. Cuando est치 "recocido", imita tus datos de entrenamiento *demasiado*, y comienza a distorsionar tus im치genes. Si tu dataset o par치metros estan mal, 춰puede que est칠 crudo y recocido al mismo tiempo!

   Usando lo aprendido en [X/Y/Z Plot 郊쑇(#plot), podemos hacer una comparaci칩n del progreso de nuestro Lora:

   ![Comparaci칩n del resultado de Lora](images/loratrain.png)

   Mira eso, 춰se vuelve cada vez m치s detallado! La 칰ltima imagen no tiene ning칰n Lora para comparar. Este parece ser un Lora de personaje exitoso, pero necesitar칤amos probar una variedad de seeds, ropas y escenas para estar seguros.

   * Si tus resultados no funcionan, puede que hayas entrenado muy poco tiempo o m치s probablemente tu tasa de aprendizaje era muy peque침a (intenta 5e-4 o en casos extremos 1e-3).
   * Si tus resultados est치n distorsionados, intenta bajar la intensidad de tu lora entre 0.5 y 0.8. Si siguen distorsionados o deja de funcionar, y epochs anteriores tampoco funcionan, entonces se te quem칩 el Lora y debes intentar una menor tasa de aprendizaje (1e-4 o 1e-5).
   * Si funciona bien pero tu personaje no puede cambiar de ropa/posici칩n, tus im치genes de entrenamiento eran muy similares o sus tags estaban mal.
   * Si funciona bien pero el estilo se ve mal o con mal sombreado, puede que est칠s usando un modelo muy avanzado. Recomiendo `animefull-final-pruned` si puedes encontrarlo.
   
   Despu칠s de acostumbrarse a hacer Loras, e interactuar on la comunidad y sus variados recursos, estar치s listo para usar otro m칠todo m치s avanzado como el [colab original todo-en-uno de kohya](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb). Buena suerte.

* **Consejos adicionales** <a name="trainchars"></a>[郊쑇(#index)

   La parte m치s importante para un personaje son los tags. Claro que necesitas im치genes con variadas poses y lugares, pero si las descripciones est치n mal no servir치 de nada.

   Cuando entrenas un personaje o concepto deber칤as definir una **palabra de activaci칩n**, y ajustar el valor de `keep_tokens` a 1. Una palabra de activaci칩n es como podremos invocar a tu Lora para que funcione. Habiendo hecho eso, **algunas personas recomiendan** quitar o "limpiar" las tags que son intr칤nsicas a tu personaje o concepto, tales como el color de pelo y ojos. Por ejemplo, si una chica siempre tiene orejas de gato, quieres quitar las tags tales como `animal ears, animal ear fluff, cat ears`, y as칤 칠stas ser치n "absorbidas" por tu palabra de activaci칩n. Esto har치 tu Lora m치s f치cil de usar pero menos flexible.
   * Puedes usar [la extensi칩n Tag Editor](https://github.com/toshiaki1729/stable-diffusion-webui-dataset-tag-editor) para a침adir una palabra de activaci칩n a todos tus archivos al mismo tiempo. Es un poco complejo: A침ade la extensi칩n y reinicia tu webui, luego entra a la nueva pesta침a **Dataset Tag Editor** y luego a *Batch Edit Captions*. Bajo *Edit Tags* escribe tu palabra de activaci칩n y enciende *Prepend additional tags*. Luego aplica los cambios (bot칩n naranja), sube y guarda (bot칩n naranja).

   Esta "absorci칩n" de detalles no entregados por los tags es la forma en que los Loras funcionan en general, ya que logran aprender y representar los detalles imperceptibles o dif칤ciles de explicar tales como el rostro, acccesorios, composici칩n, etc. Tambi칠n puedes limpiar tags de ropa redundantes, como borrar "red tie" y dejar "tie". Incluso puedes tener una palabra de activaci칩n distinta para cada atuendo que tu personaje usa com칰nmente, como personaje-normal, personaje-bikini, etc. Pero no es la 칰nica forma de hacerlo. En cualquier caso, si tus tags son correctas, tu personaje deber칤a poder cambiar de ropa f치cilmente.

   Mientras tanto, los Loras de estilo no necesitan palabra de activaci칩n, ya que deseamos que siempre est칠n activos. Absorber치n el estilo art칤stico de forma natural, y funcionar치 con variados pesos.

   Otro consejo es tener m치s de una carpeta de repeticiones, y separarlas por calidad: Dar m치s repeticiones a tus mejores im치genes, y menos repeticiones (incluso 1) a las im치genes que quiz치 tienen peor estilo pero a칰n as칤 aportan algo 칰nico.

&nbsp;

# ...vtubers? <a name="vtubers"></a>[郊쑇(#index)

Y as칤 llegamos la final de la gu칤a. Gracias por leer. Si tienes correcciones o contribuciones puedes abrir un Issue o un Pull Request en esta p치gina y echar칠 un vistazo pronto.

Tengo [otra p치gina dedicada a Loras de vtubers, en especial Hololive](https://huggingface.co/hollowstrawberry/holotard). Si es que es de tu inter칠s.

Saludos.

&nbsp;